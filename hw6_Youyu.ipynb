{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>IE 7300: Statistical Learning for Engineering</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>HW 6</center>\n",
    "<center>Youyu Zhang</center>\n",
    "<center>zhang.youy@northeastern.edu</center>\n",
    "<center>(530)574-2826</center>\n",
    "<center>Submitted by 10/25/2022</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [3,2,1,2,7,2]\n",
    "\n",
    "def minUniqueSum(arr):\n",
    "    outputList = {}\n",
    "    for i in arr:\n",
    "        while i in outputList:\n",
    "            i += 1\n",
    "        outputList[i] = 1\n",
    "    return sum(outputList.keys())\n",
    "\n",
    "def minUniqueSum2():\n",
    "    arr = input()\n",
    "    arr = arr.split()\n",
    "    inputList = []\n",
    "    for i in arr:\n",
    "        inputList.append(int(i))\n",
    "\n",
    "    outputList = {}\n",
    "    for i in inputList:\n",
    "        while i in outputList:\n",
    "            i += 1\n",
    "        outputList[i] = 1\n",
    "    return sum(outputList.keys())\n",
    "\n",
    "\n",
    "\n",
    "minUniqueSum2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 3, 5]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = '[1, 4, 3 ,5]'\n",
    "arr = arr[1:-1]\n",
    "arr = arr.split(',')\n",
    "inputList = []\n",
    "for i in arr:\n",
    "    inputList.append(int(i.strip()))\n",
    "\n",
    "inputList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118264581564861424"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def possibleWays():\n",
    "    string_n = input()  \n",
    "    string_m = input() \n",
    "    n = int(string_n)\n",
    "    m = int(string_m)\n",
    "    # (n,m) indicated the location of the point\n",
    "    cache = [[0 for col in range(m+1)] for row in range(n+1)]\n",
    "    def value(x,y):\n",
    "        if x == 0:\n",
    "            return 1\n",
    "        if y == 0:\n",
    "            return 1\n",
    "        if cache[x][y] > 0:\n",
    "            return cache[x][y]\n",
    "        else:\n",
    "            cache[x][y] = value(x-1,y)+value(x,y-1)\n",
    "            return cache[x][y]\n",
    "\n",
    "    return value(n,m)\n",
    "\n",
    "possibleWays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3660/1850108974.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mrecordedNodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxNetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxNetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3660/1850108974.py\u001b[0m in \u001b[0;36msolution\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mpair1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpair2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpair1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpair2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "def solution():\n",
    "    t = int(input())\n",
    "    for i in range(t):\n",
    "        # 1. Read Inputs\n",
    "        m = int(input())\n",
    "        matrix = []\n",
    "        for j in range(m):\n",
    "            pair1, pair2 = input().split()\n",
    "            matrix.append([int(pair1),int(pair2)])\n",
    "        \n",
    "        # 2. Preprocessing\n",
    "        recordedNodes = {}\n",
    "        preprocessing(recordedNodes)\n",
    "\n",
    "        # 3. BFS to search maximum social network within country T\n",
    "        searchedNodes = []\n",
    "        maxNetwork = 0\n",
    "        for i in recordedNodes:\n",
    "            currentNetwork = searchNetworkBFS(i)\n",
    "            if currentNetwork > maxNetwork:\n",
    "                maxNetwork = currentNetwork\n",
    "\n",
    "        # 4. Print Result for country T\n",
    "        print(maxNetwork)\n",
    "\n",
    "    class Node:\n",
    "        \"\"\"\n",
    "        This class is used to record each user as a node.\n",
    "        Node.id is the user number. \n",
    "        Node.following is a list of user numbers to which this node is connected.\n",
    "        \"\"\"\n",
    "        def __init__(self, id):\n",
    "            self.id = id\n",
    "            self.following = []\n",
    "        \n",
    "        def addFollowing(self, following):\n",
    "            # following: a single NodeId to append\n",
    "            if following not in self.following:\n",
    "                self.following.append(following)\n",
    "\n",
    "    # The following part converts matrix to nodes relationships\n",
    "    def preprocessing(recordedNodes):\n",
    "        for i in range(len(matrix)):\n",
    "            # Check if the id has been recorded before\n",
    "            # If no, initiate a new one\n",
    "            for j in matrix[i]:\n",
    "                if j not in recordedNodes:\n",
    "                    recordedNodes[j] = Node(id=j)\n",
    "            # Add following relationship\n",
    "            nodeId1 = matrix[i][0]\n",
    "            nodeId2 = matrix[i][1]\n",
    "            recordedNodes[nodeId1].addFollowing(following=nodeId2)\n",
    "            recordedNodes[nodeId2].addFollowing(following=nodeId1)\n",
    "\n",
    "    def searchNetworkBFS(nodeId):\n",
    "        if nodeId in searchedNodes:\n",
    "            return 0\n",
    "        temp = 0\n",
    "        currentQueue = [nodeId]\n",
    "        while len(currentQueue) > 0:\n",
    "            currentNode = currentQueue.pop(0)\n",
    "            searchedNodes.append(currentNode)\n",
    "            temp += 1\n",
    "            following = recordedNodes[currentNode].following\n",
    "            for i in following:\n",
    "                if i not in searchedNodes and i not in currentQueue:\n",
    "                    currentQueue.append(i)\n",
    "        return temp\n",
    "\n",
    "solution()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['b', 'a', 'c', 1, [2]]\n",
      "['b', 'a', 'c', [2], 1]\n",
      "['b', 'a', 1, 'c', [2]]\n",
      "['b', 'a', 1, [2], 'c']\n",
      "['b', 'a', [2], 'c', 1]\n",
      "['b', 'a', [2], 1, 'c']\n",
      "['b', 'c', 'a', 1, [2]]\n",
      "['b', 'c', 'a', [2], 1]\n",
      "['b', 'c', 1, 'a', [2]]\n",
      "['b', 'c', 1, [2], 'a']\n",
      "['b', 'c', [2], 'a', 1]\n",
      "['b', 'c', [2], 1, 'a']\n",
      "['b', 1, 'a', 'c', [2]]\n",
      "['b', 1, 'a', [2], 'c']\n",
      "['b', 1, 'c', 'a', [2]]\n",
      "['b', 1, 'c', [2], 'a']\n",
      "['b', 1, [2], 'a', 'c']\n",
      "['b', 1, [2], 'c', 'a']\n",
      "['b', [2], 'a', 'c', 1]\n",
      "['b', [2], 'a', 1, 'c']\n",
      "['b', [2], 'c', 'a', 1]\n",
      "['b', [2], 'c', 1, 'a']\n",
      "['b', [2], 1, 'a', 'c']\n",
      "['b', [2], 1, 'c', 'a']\n",
      "['a', 'b', 'c', 1, [2]]\n",
      "['a', 'b', 'c', [2], 1]\n",
      "['a', 'b', 1, 'c', [2]]\n",
      "['a', 'b', 1, [2], 'c']\n",
      "['a', 'b', [2], 'c', 1]\n",
      "['a', 'b', [2], 1, 'c']\n",
      "['a', 'c', 'b', 1, [2]]\n",
      "['a', 'c', 'b', [2], 1]\n",
      "['a', 'c', 1, 'b', [2]]\n",
      "['a', 'c', 1, [2], 'b']\n",
      "['a', 'c', [2], 'b', 1]\n",
      "['a', 'c', [2], 1, 'b']\n",
      "['a', 1, 'b', 'c', [2]]\n",
      "['a', 1, 'b', [2], 'c']\n",
      "['a', 1, 'c', 'b', [2]]\n",
      "['a', 1, 'c', [2], 'b']\n",
      "['a', 1, [2], 'b', 'c']\n",
      "['a', 1, [2], 'c', 'b']\n",
      "['a', [2], 'b', 'c', 1]\n",
      "['a', [2], 'b', 1, 'c']\n",
      "['a', [2], 'c', 'b', 1]\n",
      "['a', [2], 'c', 1, 'b']\n",
      "['a', [2], 1, 'b', 'c']\n",
      "['a', [2], 1, 'c', 'b']\n",
      "['c', 'b', 'a', 1, [2]]\n",
      "['c', 'b', 'a', [2], 1]\n",
      "['c', 'b', 1, 'a', [2]]\n",
      "['c', 'b', 1, [2], 'a']\n",
      "['c', 'b', [2], 'a', 1]\n",
      "['c', 'b', [2], 1, 'a']\n",
      "['c', 'a', 'b', 1, [2]]\n",
      "['c', 'a', 'b', [2], 1]\n",
      "['c', 'a', 1, 'b', [2]]\n",
      "['c', 'a', 1, [2], 'b']\n",
      "['c', 'a', [2], 'b', 1]\n",
      "['c', 'a', [2], 1, 'b']\n",
      "['c', 1, 'b', 'a', [2]]\n",
      "['c', 1, 'b', [2], 'a']\n",
      "['c', 1, 'a', 'b', [2]]\n",
      "['c', 1, 'a', [2], 'b']\n",
      "['c', 1, [2], 'b', 'a']\n",
      "['c', 1, [2], 'a', 'b']\n",
      "['c', [2], 'b', 'a', 1]\n",
      "['c', [2], 'b', 1, 'a']\n",
      "['c', [2], 'a', 'b', 1]\n",
      "['c', [2], 'a', 1, 'b']\n",
      "['c', [2], 1, 'b', 'a']\n",
      "['c', [2], 1, 'a', 'b']\n",
      "[1, 'b', 'a', 'c', [2]]\n",
      "[1, 'b', 'a', [2], 'c']\n",
      "[1, 'b', 'c', 'a', [2]]\n",
      "[1, 'b', 'c', [2], 'a']\n",
      "[1, 'b', [2], 'a', 'c']\n",
      "[1, 'b', [2], 'c', 'a']\n",
      "[1, 'a', 'b', 'c', [2]]\n",
      "[1, 'a', 'b', [2], 'c']\n",
      "[1, 'a', 'c', 'b', [2]]\n",
      "[1, 'a', 'c', [2], 'b']\n",
      "[1, 'a', [2], 'b', 'c']\n",
      "[1, 'a', [2], 'c', 'b']\n",
      "[1, 'c', 'b', 'a', [2]]\n",
      "[1, 'c', 'b', [2], 'a']\n",
      "[1, 'c', 'a', 'b', [2]]\n",
      "[1, 'c', 'a', [2], 'b']\n",
      "[1, 'c', [2], 'b', 'a']\n",
      "[1, 'c', [2], 'a', 'b']\n",
      "[1, [2], 'b', 'a', 'c']\n",
      "[1, [2], 'b', 'c', 'a']\n",
      "[1, [2], 'a', 'b', 'c']\n",
      "[1, [2], 'a', 'c', 'b']\n",
      "[1, [2], 'c', 'b', 'a']\n",
      "[1, [2], 'c', 'a', 'b']\n",
      "[[2], 'b', 'a', 'c', 1]\n",
      "[[2], 'b', 'a', 1, 'c']\n",
      "[[2], 'b', 'c', 'a', 1]\n",
      "[[2], 'b', 'c', 1, 'a']\n",
      "[[2], 'b', 1, 'a', 'c']\n",
      "[[2], 'b', 1, 'c', 'a']\n",
      "[[2], 'a', 'b', 'c', 1]\n",
      "[[2], 'a', 'b', 1, 'c']\n",
      "[[2], 'a', 'c', 'b', 1]\n",
      "[[2], 'a', 'c', 1, 'b']\n",
      "[[2], 'a', 1, 'b', 'c']\n",
      "[[2], 'a', 1, 'c', 'b']\n",
      "[[2], 'c', 'b', 'a', 1]\n",
      "[[2], 'c', 'b', 1, 'a']\n",
      "[[2], 'c', 'a', 'b', 1]\n",
      "[[2], 'c', 'a', 1, 'b']\n",
      "[[2], 'c', 1, 'b', 'a']\n",
      "[[2], 'c', 1, 'a', 'b']\n",
      "[[2], 1, 'b', 'a', 'c']\n",
      "[[2], 1, 'b', 'c', 'a']\n",
      "[[2], 1, 'a', 'b', 'c']\n",
      "[[2], 1, 'a', 'c', 'b']\n",
      "[[2], 1, 'c', 'b', 'a']\n",
      "[[2], 1, 'c', 'a', 'b']\n"
     ]
    }
   ],
   "source": [
    "# matrix = [[2,3], [4,5],[5,7], [3,8],[8,1],\n",
    "#           [2,6], [1,6]]\n",
    "\n",
    "def solution():\n",
    "    t = int(input())\n",
    "    for i in range(t):\n",
    "        m = int(input())\n",
    "        matrix = []\n",
    "        for j in range(m):\n",
    "            pair1, pair2 = input().split()\n",
    "            matrix.append([int(pair1),int(pair2)])\n",
    "\n",
    "    class Node:\n",
    "        \"\"\"\n",
    "        This class is used to record each user as a node.\n",
    "        Node.id is the user number. \n",
    "        Node.following is a list of user numbers to which this node is connected.\n",
    "        \"\"\"\n",
    "        def __init__(self, id):\n",
    "            self.id = id\n",
    "            self.following = []\n",
    "        \n",
    "        def addFollowing(self, following):\n",
    "            # following: a single NodeId to append\n",
    "            if following not in self.following:\n",
    "                self.following.append(following)\n",
    "\n",
    "    # The following part converts matrix to nodes relationships\n",
    "    recordedNodes = {}\n",
    "    for i in range(len(matrix)):\n",
    "        # Check if the id has been recorded before\n",
    "        # If no, initiate a new one\n",
    "        for j in matrix[i]:\n",
    "            if j not in recordedNodes:\n",
    "                recordedNodes[j] = Node(id=j)\n",
    "        # Add following relationship\n",
    "        nodeId1 = matrix[i][0]\n",
    "        nodeId2 = matrix[i][1]\n",
    "        recordedNodes[nodeId1].addFollowing(following=nodeId2)\n",
    "        recordedNodes[nodeId2].addFollowing(following=nodeId1)\n",
    "\n",
    "    # The following part searches the maximum network\n",
    "    searchedNodes = []\n",
    "    maxNetwork = 0\n",
    "\n",
    "    def searchNetwork(nodeId):\n",
    "        temp = 0\n",
    "        if nodeId not in searchedNodes:\n",
    "            searchedNodes.append(nodeId)\n",
    "            temp += 1\n",
    "            for i in recordedNodes[nodeId].following:\n",
    "                temp += searchNetwork(i)\n",
    "        return temp\n",
    "\n",
    "    \n",
    "    def searchNetwork2(nodeId):\n",
    "        if nodeId in searchedNodes:\n",
    "            return 0\n",
    "        temp = 0\n",
    "        currentQueue = [nodeId]\n",
    "        while len(currentQueue) > 0:\n",
    "            currentNode = currentQueue.pop(0)\n",
    "            searchedNodes.append(currentNode)\n",
    "            temp += 1\n",
    "            following = recordedNodes[currentNode].following\n",
    "            for i in following:\n",
    "                if i not in searchedNodes and i not in currentQueue:\n",
    "                    currentQueue.append(i)\n",
    "        return temp\n",
    "\n",
    "    # def searchNetwork3(nodeId):\n",
    "    #     if nodeId in searchedNodes:\n",
    "    #         return 0\n",
    "    #     temp = 0\n",
    "    #     currentStack = [nodeId]\n",
    "    #     searchedNodes.append(nodeId)\n",
    "    #     while len(currentStack) > 0:\n",
    "    #         currentNode = currentStack[-1]\n",
    "    #         following = recordedNodes[currentNode].following\n",
    "    #         for i in following:\n",
    "    #             if i not in searchedNodes and i not in currentStack:\n",
    "    #                 currentStack.append(i)\n",
    "                \n",
    "    def searchNetwork3(nodeId):\n",
    "        if nodeId in searchedNodes:\n",
    "            return 0\n",
    "        temp = 1\n",
    "        currentStack = [[nodeId, 0]]\n",
    "        while len(currentStack) > 0:\n",
    "            current = currentStack[-1]\n",
    "            searchedNodes.append(current[0])\n",
    "            following = recordedNodes[current[0]].following\n",
    "            if current[1] < len(following):\n",
    "                nextNode = following[current[1]]\n",
    "                if nextNode not in searchedNodes:\n",
    "                    currentStack.append([nextNode, 0])\n",
    "                    temp += 1\n",
    "                current[1] += 1\n",
    "            else:\n",
    "                currentStack.pop()\n",
    "        return temp\n",
    "\n",
    "    for i in recordedNodes:\n",
    "        currentNetwork = searchNetwork3(i)\n",
    "        if currentNetwork > maxNetwork:\n",
    "            maxNetwork = currentNetwork\n",
    "\n",
    "    return recordedNodes, maxNetwork\n",
    "       \n",
    "def permutation(alphabets):\n",
    "    currentStack = [alphabets[0]]\n",
    "    while len(currentStack) > 0:\n",
    "        if len(currentStack) < len(alphabets):\n",
    "            tempIndex = 0\n",
    "            while alphabets[tempIndex] in currentStack:\n",
    "                tempIndex += 1\n",
    "            if tempIndex < len(alphabets):\n",
    "                currentStack.append(alphabets[tempIndex])\n",
    "        else:\n",
    "            print(currentStack)\n",
    "            candidateAlphabet = getNextAlphabet(alphabets, currentStack[-1], currentStack)\n",
    "            while candidateAlphabet == '' and len(currentStack) > 0:\n",
    "                currentStack.pop()\n",
    "                if len(currentStack) > 0:\n",
    "                    candidateAlphabet = getNextAlphabet(alphabets, currentStack[-1], currentStack)\n",
    "            if len(currentStack) > 0:\n",
    "                currentStack[-1] = candidateAlphabet\n",
    "\n",
    "def getNextAlphabet(alphabets, current, stack):\n",
    "    index = alphabets.index(current) + 1\n",
    "    while index < len(alphabets) and alphabets[index] in stack:\n",
    "        index += 1\n",
    "    if index >= len(alphabets):\n",
    "        return ''\n",
    "    else:\n",
    "        return alphabets[index]\n",
    "\n",
    "recordedNodes, maxNetwork = solution(matrix)\n",
    "print(maxNetwork)\n",
    "permutation(['b','a','c',1,[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, [2, 3]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class A:\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "temp = A(1,2)\n",
    "temp.a = 0\n",
    "\n",
    "\n",
    "a = [1, [2, 3]]\n",
    "c = a[1]\n",
    "c = [2,2]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and test dataset 80:20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wine</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic.acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Acl</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid.phenols</th>\n",
       "      <th>Proanth</th>\n",
       "      <th>Color.int</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wine  Alcohol  Malic.acid   Ash   Acl   Mg  Phenols  Flavanoids  \\\n",
       "0     1    14.23        1.71  2.43  15.6  127     2.80        3.06   \n",
       "1     1    13.20        1.78  2.14  11.2  100     2.65        2.76   \n",
       "2     1    13.16        2.36  2.67  18.6  101     2.80        3.24   \n",
       "\n",
       "   Nonflavanoid.phenols  Proanth  Color.int   Hue    OD  Proline  \n",
       "0                  0.28     2.29       5.64  1.04  3.92     1065  \n",
       "1                  0.26     1.28       4.38  1.05  3.40     1050  \n",
       "2                  0.30     2.81       5.68  1.03  3.17     1185  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('wine-1.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset length : 142\n",
      "Testing dataset length : 36\n"
     ]
    }
   ],
   "source": [
    "x_all,y_all = np.array(df.drop('Wine', axis=1)), np.array(df['Wine'])\n",
    "x_train, x_test = np.split(x_all,[int(0.8*len(x_all))])\n",
    "y_train, y_test = np.split(y_all,[int(0.8*len(y_all))])\n",
    "\n",
    "print('Training dataset length :', x_train.shape[0])\n",
    "print('Testing dataset length :', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Create a logistics regression, LDA, decision tree, and AdaBoost models. Fit the model using the training dataset and find the model accuracy and confusion matrix. Explain each model's outcome, finding, and accuracy. (10 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, epochs: int=2000, threshold: float=1e-3) -> None:\n",
    "        self.epochs = epochs\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def train(self, x, y, batch_size=64, lr=0.001):\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_labels = {c:i for i,c in enumerate(self.classes)}\n",
    "        x = np.insert(x, 0, 1, axis=1)\n",
    "        y = np.eye(len(self.classes))[np.vectorize(lambda c: self.class_labels[c])(y).reshape(-1)]\n",
    "        self.loss = []\n",
    "        self.weights = np.zeros(shape=(len(self.classes),x.shape[1]))\n",
    "        i = 0\n",
    "        while (not self.epochs or i < self.epochs):\n",
    "            self.loss.append(self.cross_entropy(y, self.predict_(x)))\n",
    "            idx = np.random.choice(x.shape[0], batch_size)\n",
    "            x_batch, y_batch = x[idx], y[idx]\n",
    "            error = y_batch - self.predict_(x_batch)\n",
    "            update = (lr * np.dot(error.T, x_batch))\n",
    "            self.weights = np.add(self.weights, update)\n",
    "            # self.weights += update\n",
    "            if np.abs(update).max() < self.threshold: \n",
    "                break\n",
    "            if i % (self.epochs/5)  == 0: \n",
    "                print(' Training Accuray at {} iterations is {}'.format(i, self.evaluate_(x, y)))\n",
    "            i +=1\n",
    "        return self\n",
    "\n",
    "    def predict_(self, X):\n",
    "        pre_vals = np.dot(X, self.weights.T).reshape(-1,len(self.classes))\n",
    "        return self.softmax(pre_vals)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1, axis=1)\n",
    "        pre_vals = np.dot(x, self.weights.T).reshape(-1,len(self.classes))\n",
    "        return self.softmax(pre_vals)\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        return np.exp(z.astype(float))/np.sum(np.exp(z.astype(float)), axis=1).reshape(-1,1)\n",
    "    \n",
    "    def evaluate_(self, X, y):\n",
    "        return np.mean(np.argmax(self.predict_(X), axis=1) == np.argmax(y, axis=1))\n",
    "\n",
    "    def cross_entropy(self, y, probs):\n",
    "        return -1 * np.mean(y * np.log(probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, var_red=None, value=None):\n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.var_red = var_red\n",
    "        \n",
    "        # for leaf node\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    This class includes two different decision tree method:\n",
    "    1. regressor, methodName \n",
    "    2. classification, methodName_\n",
    "    \"\"\"\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        # initialize the root of the tree \n",
    "        self.root = None\n",
    "        # stopping conditions\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0):  \n",
    "        \"\"\"\n",
    "        Regressor build\n",
    "        \"\"\"\n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        best_split = {}\n",
    "        # split until stopping conditions are met\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"var_red\"]>0:\n",
    "                # recur left\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                # recur right\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"var_red\"])\n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "\n",
    "    def build_tree_(self, dataset, curr_depth=0):\n",
    "        \"\"\"\n",
    "        Classification build\n",
    "        \"\"\"\n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        # split until stopping conditions are met\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split_(dataset, num_samples, num_features)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"info_gain\"]>0:\n",
    "                # recur left\n",
    "                left_subtree = self.build_tree_(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                # recur right\n",
    "                right_subtree = self.build_tree_(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value_(Y)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        ''' function to find the best split '''\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_var_red = -float(\"inf\")\n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # compute information gain\n",
    "                    curr_var_red = self.variance_reduction(y, left_y, right_y)\n",
    "                    # update the best split if needed\n",
    "                    if curr_var_red>max_var_red:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"var_red\"] = curr_var_red\n",
    "                        max_var_red = curr_var_red\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    def get_best_split_(self, dataset, num_samples, num_features):\n",
    "        ''' function to find the best split '''\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # compute information gain\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "                    # update the best split if needed\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "\n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    def variance_reduction(self, parent, l_child, r_child):\n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        reduction = np.var(parent) - (weight_l * np.var(l_child) + weight_r * np.var(r_child))\n",
    "        return reduction\n",
    "\n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
    "        ''' function to compute information gain '''\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode==\"gini\":\n",
    "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        return gain\n",
    "\n",
    "    def entropy(self, y):\n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "\n",
    "    def gini_index(self, y):\n",
    "        ''' function to compute gini index '''\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "        \n",
    "    def calculate_leaf_value_(self, Y):\n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "\n",
    "    def calculate_leaf_value(self, Y):\n",
    "        val = np.mean(Y)\n",
    "        return val\n",
    "                \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.var_red)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fit for regression.\n",
    "        \"\"\"\n",
    "        # dataset = np.concatenate((X, Y), axis=1)\n",
    "        dataset = np.concatenate((X,np.array(Y).reshape(len(Y),1)),axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "        \n",
    "    def fit_(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fit for classification.\n",
    "        \"\"\"\n",
    "        # dataset = np.concatenate((X, Y), axis=1)\n",
    "        dataset = np.concatenate((X,np.array(Y).reshape(len(Y),1)),axis=1)\n",
    "        self.root = self.build_tree_(dataset)\n",
    "\n",
    "    def make_prediction(self, x, tree):\n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n",
    "    \n",
    "    def predict(self, X): \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Predict the models using the test dataset, and provide the performance metrics. Compare the five models' performance metrics, and explain at least four findings on each of the models. Do not need to repeat the code). (10 points)\n",
    "\n",
    "c) Do you see any overfitting and underfitting issues on each model? How to overcome the overfitting/underfitting issues in your custom models? Provide the information model by model. (10 points)\n",
    "\n",
    "d) List down the important features in each model? How did you choose those important features? Why is it an important feature of the model? Explain with some statistical evidence. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82f6ad5c16f96bd4c09885635dc45a9905d4ac7b1c081542d45b0fe5279d5fc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
