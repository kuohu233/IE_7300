{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>IE 7300: Statistical Learning for Engineering</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>HW 6</center>\n",
    "<center>Youyu Zhang</center>\n",
    "<center>zhang.youy@northeastern.edu</center>\n",
    "<center>(530)574-2826</center>\n",
    "<center>Submitted by 10/25/2022</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and test dataset 80:20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wine</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic.acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Acl</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid.phenols</th>\n",
       "      <th>Proanth</th>\n",
       "      <th>Color.int</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wine  Alcohol  Malic.acid   Ash   Acl   Mg  Phenols  Flavanoids  \\\n",
       "0     1    14.23        1.71  2.43  15.6  127     2.80        3.06   \n",
       "1     1    13.20        1.78  2.14  11.2  100     2.65        2.76   \n",
       "2     1    13.16        2.36  2.67  18.6  101     2.80        3.24   \n",
       "\n",
       "   Nonflavanoid.phenols  Proanth  Color.int   Hue    OD  Proline  \n",
       "0                  0.28     2.29       5.64  1.04  3.92     1065  \n",
       "1                  0.26     1.28       4.38  1.05  3.40     1050  \n",
       "2                  0.30     2.81       5.68  1.03  3.17     1185  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('wine-1.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset length : 142\n",
      "Testing dataset length : 36\n"
     ]
    }
   ],
   "source": [
    "x_all,y_all = np.array(df.drop('Wine', axis=1)), np.array(df['Wine'])\n",
    "x_train, x_test = np.split(x_all,[int(0.8*len(x_all))])\n",
    "y_train, y_test = np.split(y_all,[int(0.8*len(y_all))])\n",
    "\n",
    "print('Training dataset length :', x_train.shape[0])\n",
    "print('Testing dataset length :', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Create a logistics regression, LDA, decision tree, and AdaBoost models. Fit the model using the training dataset and find the model accuracy and confusion matrix. Explain each model's outcome, finding, and accuracy. (10 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic regression for multi-class classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, epochs: int=2000, threshold: float=1e-3) -> None:\n",
    "        \"\"\"\n",
    "        epochs: iteration times\n",
    "        threshold: threshold when the change of weights are less than\n",
    "            this number, the iteration can stop.\n",
    "        \"\"\"\n",
    "        self.epochs = epochs\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def train(self, x, y, batch_size=64, lr=0.001):\n",
    "        \"\"\"\n",
    "        Model training. \n",
    "        lr: learning rate\n",
    "        batch_size: sample size\n",
    "        \"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_labels = {c:i for i,c in enumerate(self.classes)}\n",
    "        # x = np.insert(x, 0, 1, axis=1)\n",
    "        y = np.eye(len(self.classes))[np.vectorize(lambda c: self.class_labels[c])(y).reshape(-1)]\n",
    "        self.loss = []\n",
    "        self.weights = np.zeros(shape=(len(self.classes),x.shape[1]))\n",
    "        i = 0\n",
    "        while (not self.epochs or i < self.epochs):\n",
    "            self.loss.append(self.cross_entropy(y, self.predict(x)))\n",
    "            idx = np.random.choice(x.shape[0], batch_size)\n",
    "            x_batch, y_batch = x[idx], y[idx]\n",
    "            error = y_batch - self.predict(x_batch)\n",
    "            update = (lr * np.dot(error.T, x_batch))\n",
    "            self.weights = np.add(self.weights, update)\n",
    "            # self.weights += update\n",
    "            if np.abs(update).max() < self.threshold: \n",
    "                break\n",
    "            if i % (self.epochs/5)  == 0: \n",
    "                print(' Training Accuray at {} iterations is {}'.format(i, self.evaluate_(x, y)))\n",
    "            i +=1\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict with given X. Return the softmax value.\n",
    "        \"\"\"\n",
    "        # X = np.insert(X, 0, 1, axis=1)\n",
    "        pre_vals = np.dot(X, self.weights.T).reshape(-1,len(self.classes))\n",
    "        return self.softmax(pre_vals)\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        return np.exp(z.astype(float))/np.sum(np.exp(z.astype(float)), axis=1).reshape(-1,1)\n",
    "    \n",
    "    def evaluate_(self, X, y):\n",
    "        return np.mean(np.argmax(self.predict(X), axis=1) == np.argmax(y, axis=1))\n",
    "\n",
    "    def cross_entropy(self, y, probs):\n",
    "        return -1 * np.mean(y * np.log(probs))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDiscriminantAnalysis:\n",
    "    \"\"\"\n",
    "    LDA\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components):\n",
    "        \"\"\"\n",
    "        n_components: the number of components to yield\n",
    "        linear_discriminants: eigenvector of the n_components\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.linear_discriminants = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        class_labels = np.unique(y)\n",
    "\n",
    "        mean_overall = np.mean(X, axis=0)\n",
    "        # SW indicated within class scattering\n",
    "        SW = np.zeros((n_features, n_features))\n",
    "        # SB indicated the scattering between classes\n",
    "        SB = np.zeros((n_features, n_features))\n",
    "\n",
    "        for c in class_labels:\n",
    "            X_c = X[y == c]\n",
    "            mean_c = np.mean(X_c, axis=0)\n",
    "            SW += (X_c - mean_c).T.dot((X_c - mean_c))\n",
    "            n_c = X_c.shape[0]\n",
    "            mean_diff = (mean_c - mean_overall).reshape(n_features, 1)\n",
    "            SB += n_c * (mean_diff).dot(mean_diff.T)\n",
    "\n",
    "        # Determine SW^-1 * SB\n",
    "        A = np.linalg.inv(SW).dot(SB)\n",
    "        # Get eigenvalues and eigenvectors of SW^-1 * SB\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "        # -> eigenvector v = [:,i] column vector, transpose for easier calculations\n",
    "        # sort eigenvalues high to low\n",
    "        eigenvectors = eigenvectors.T\n",
    "        idxs = np.argsort(abs(eigenvalues))[::-1]\n",
    "        eigenvalues = eigenvalues[idxs]\n",
    "        eigenvectors = eigenvectors[idxs]\n",
    "        # store first n eigenvectors\n",
    "        self.linear_discriminants = eigenvectors[0 : self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        # project data\n",
    "        return np.dot(X, self.linear_discriminants.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, var_red=None, value=None):\n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.var_red = var_red\n",
    "        \n",
    "        # for leaf node\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    This class includes two different decision tree method:\n",
    "    1. regressor, methodName \n",
    "    2. classification, methodName_\n",
    "    \"\"\"\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        # initialize the root of the tree \n",
    "        self.root = None\n",
    "        # stopping conditions\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0):  \n",
    "        \"\"\"\n",
    "        Regressor build\n",
    "        \"\"\"\n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        best_split = {}\n",
    "        # split until stopping conditions are met\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"var_red\"]>0:\n",
    "                # recur left\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                # recur right\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"var_red\"])\n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "\n",
    "    def build_tree_(self, dataset, curr_depth=0,sample_weight=None):\n",
    "        \"\"\"\n",
    "        Classification build\n",
    "        \"\"\"\n",
    "        if sample_weight is not None:\n",
    "            X, Y, sample_weight = dataset[:,:-2], dataset[:,-2], dataset[-1]\n",
    "        else:\n",
    "            X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        # split until stopping conditions are met\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split_(dataset, num_samples, num_features,sample_weight=None)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"info_gain\"]>0:\n",
    "                # recur left\n",
    "                left_subtree = self.build_tree_(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                # recur right\n",
    "                right_subtree = self.build_tree_(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value_(Y)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        ''' function to find the best split '''\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_var_red = -float(\"inf\")\n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # compute information gain\n",
    "                    curr_var_red = self.variance_reduction(y, left_y, right_y)\n",
    "                    # update the best split if needed\n",
    "                    if curr_var_red>max_var_red:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"var_red\"] = curr_var_red\n",
    "                        max_var_red = curr_var_red\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    def get_best_split_(self, dataset, num_samples, num_features,sample_weight=None):\n",
    "        ''' function to find the best split '''\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    if sample_weight == None:\n",
    "                        y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                        curr_info_gain = self.information_gain(\n",
    "                        parent=y,\n",
    "                        l_child=left_y,\n",
    "                        r_child=right_y,\n",
    "                        mode=\"gini\",\n",
    "                        sample_weight=None\n",
    "                        )\n",
    "                    else:\n",
    "                        y, left_y, right_y = dataset[:, -2], dataset_left[:, -2], dataset_right[:, -2]\n",
    "                        weight, left_weight, right_weight = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                        # compute information gain\n",
    "                        curr_info_gain = self.information_gain(\n",
    "                            parent=y,\n",
    "                            l_child=left_y,\n",
    "                            r_child=right_y,\n",
    "                            mode=\"gini\",\n",
    "                            sample_weight=[weight, left_weight, right_weight]\n",
    "                            )\n",
    "                    # update the best split if needed\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "\n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    def variance_reduction(self, parent, l_child, r_child):\n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        reduction = np.var(parent) - (weight_l * np.var(l_child) + weight_r * np.var(r_child))\n",
    "        return reduction\n",
    "\n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\",sample_weight=None):\n",
    "        ''' function to compute information gain '''\n",
    "        if sample_weight == None:\n",
    "            weight_l = len(l_child) / len(parent)\n",
    "            weight_r = len(r_child) / len(parent)\n",
    "            if mode==\"gini\":\n",
    "                gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "            else:\n",
    "                gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        else:\n",
    "            parent_weight = sample_weight[0]\n",
    "            weight_l = sample_weight[1]\n",
    "            weight_r = sample_weight[2]\n",
    "            if mode==\"gini\":\n",
    "                gain = self.gini_index(parent,parent_weight) - (self.gini_index(l_child,weight_l) + self.gini_index(r_child,weight_r))\n",
    "            else:\n",
    "                gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        return gain\n",
    "\n",
    "    def entropy(self, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Function to calculate entropy\n",
    "        \"\"\"\n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "\n",
    "    def gini_index(self, y, sample_weight=None):\n",
    "        '''\n",
    "        Function to compute gini index \n",
    "        Weighted added. \n",
    "        '''\n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "\n",
    "        if sample_weight == None:\n",
    "            for cls in class_labels:\n",
    "                p_cls = len(y[y == cls]) / len(y)\n",
    "                gini += p_cls**2\n",
    "        else:  # Not weighted\n",
    "            # Check if y and sample_weight has same length\n",
    "            if len(y) != len(sample_weight):\n",
    "                print('Length of y and sample_weight do not match.')\n",
    "            # Normalize weight before calculation\n",
    "            ss = sum(sample_weight)\n",
    "            norm_weight = [i/ss for i in sample_weight]\n",
    "            for cls in class_labels:\n",
    "                ind = []\n",
    "                p_cls_items = []\n",
    "                for idx, value in enumerate(y):\n",
    "                    if value == cls:\n",
    "                        ind.append(idx)\n",
    "                        p_cls_items.append(norm_weight[idx])\n",
    "                p_cls = sum(p_cls_items)\n",
    "                gini += p_cls**2\n",
    "\n",
    "        return 1 - gini\n",
    "        \n",
    "    def calculate_leaf_value_(self, Y):\n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "\n",
    "    def calculate_leaf_value(self, Y):\n",
    "        val = np.mean(Y)\n",
    "        return val\n",
    "                \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.var_red)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "    \n",
    "    def fit_(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fit for regression.\n",
    "        \"\"\"\n",
    "        # dataset = np.concatenate((X, Y), axis=1)\n",
    "        dataset = np.concatenate((X,np.array(Y).reshape(len(Y),1)),axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "        \n",
    "    def fit(self, X, Y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Fit for classification.\n",
    "        \"\"\"\n",
    "        # dataset = np.concatenate((X, Y), axis=1)\n",
    "        dataset = np.concatenate((X,np.array(Y).reshape(len(Y),1)),axis=1)\n",
    "        if sample_weight is not None:\n",
    "            dataset = np.concatenate((dataset,np.array(sample_weight).reshape(len(sample_weight),1)),axis=1)\n",
    "            self.root = self.build_tree_(dataset, sample_weight=sample_weight)\n",
    "        self.root = self.build_tree_(dataset)\n",
    "\n",
    "    def make_prediction(self, x, tree):\n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n",
    "    \n",
    "    def predict(self, X): \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class AdaBoostClassifier(object):\n",
    "    '''\n",
    "    Parameters\n",
    "    -----------\n",
    "    base_estimator: object\n",
    "        The base model from which the boosted ensemble is built.\n",
    "    n_estimators: integer, optional(default=50)\n",
    "        The maximum number of estimators\n",
    "    learning_rate: float, optional(default=1)\n",
    "    algorithm: {'SAMME','SAMME.R'}, optional(default='SAMME.R')\n",
    "        SAMME.R uses predicted probabilities to update wights, while SAMME uses class error rate\n",
    "    random_state: int or None, optional(default=None)\n",
    "    Attributes\n",
    "    -------------\n",
    "    estimators_: list of base estimators\n",
    "    estimator_weights_: array of floats\n",
    "        Weights for each base_estimator\n",
    "    estimator_errors_: array of floats\n",
    "        Classification error for each estimator in the boosted ensemble.\n",
    "    Reference:\n",
    "    1. [multi-adaboost](https://web.stanford.edu/~hastie/Papers/samme.pdf)\n",
    "    2. [scikit-learn:weight_boosting](https://github.com/scikit-learn/\n",
    "    scikit-learn/blob/51a765a/sklearn/ensemble/weight_boosting.py#L289)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if kwargs and args:\n",
    "            raise ValueError(\n",
    "                '''AdaBoostClassifier can only be called with keyword\n",
    "                   arguments for the following keywords: base_estimator ,n_estimators,\n",
    "                    learning_rate,algorithm,random_state''')\n",
    "        allowed_keys = [\n",
    "            'base_estimator', 'n_estimators', \n",
    "            'learning_rate', 'algorithm', 'random_state'\n",
    "            ]\n",
    "        keywords_used = kwargs.keys()\n",
    "        for keyword in keywords_used:\n",
    "            if keyword not in allowed_keys:\n",
    "                raise ValueError(keyword + \":  Wrong keyword used --- check spelling\")\n",
    "\n",
    "        n_estimators = 50\n",
    "        learning_rate = 1\n",
    "        algorithm = 'SAMME.R'\n",
    "        random_state = None\n",
    "\n",
    "        if kwargs and not args:\n",
    "            if 'base_estimator' in kwargs:\n",
    "                base_estimator = kwargs.pop('base_estimator')\n",
    "            else:\n",
    "                raise ValueError('''base_estimator can not be None''')\n",
    "            if 'n_estimators' in kwargs: \n",
    "                n_estimators = kwargs.pop('n_estimators')\n",
    "            if 'learning_rate' in kwargs: \n",
    "                learning_rate = kwargs.pop('learning_rate')\n",
    "            if 'algorithm' in kwargs: \n",
    "                algorithm = kwargs.pop('algorithm')\n",
    "            if 'random_state' in kwargs: \n",
    "                random_state = kwargs.pop('random_state')\n",
    "\n",
    "        self.base_estimator_ = base_estimator\n",
    "        self.n_estimators_ = n_estimators\n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.algorithm_ = algorithm\n",
    "        self.random_state_ = random_state\n",
    "        self.estimators_ = list()\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators_)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators_)\n",
    "\n",
    "\n",
    "    def _samme_proba(self, estimator, n_classes, X):\n",
    "        \"\"\"Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
    "        \"\"\"\n",
    "        proba = estimator.predict_proba(X)\n",
    "\n",
    "        # Displace zero probabilities so the log is defined.\n",
    "        # Also fix negative elements which may occur with\n",
    "        # negative sample weights.\n",
    "        proba[proba < np.finfo(proba.dtype).eps] = np.finfo(proba.dtype).eps\n",
    "        log_proba = np.log(proba)\n",
    "\n",
    "        return (n_classes - 1) * (log_proba - (1. / n_classes)\n",
    "                                  * log_proba.sum(axis=1)[:, np.newaxis])\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_samples = X.shape[0]\n",
    "        # There is hidden trouble for classes, here the classes will be sorted.\n",
    "        # So in boost we have to ensure that the predict results have the same classes sort\n",
    "        self.classes_ = np.array(sorted(list(set(y))))\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        for iboost in range(self.n_estimators_):\n",
    "            if iboost == 0:\n",
    "                sample_weight = np.ones(self.n_samples) / self.n_samples\n",
    "\n",
    "            sample_weight, estimator_weight, estimator_error = self.boost(X, y, sample_weight)\n",
    "\n",
    "            # early stop\n",
    "            if estimator_error == None:\n",
    "                break\n",
    "\n",
    "            # append error and weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "\n",
    "            if estimator_error <= 0:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def boost(self, X, y, sample_weight):\n",
    "        if self.algorithm_ == 'SAMME':\n",
    "            return self.discrete_boost(X, y, sample_weight)\n",
    "        elif self.algorithm_ == 'SAMME.R':\n",
    "            return self.real_boost(X, y, sample_weight)\n",
    "\n",
    "    def real_boost(self, X, y, sample_weight):\n",
    "        estimator = deepcopy(self.base_estimator_)\n",
    "        if self.random_state_:\n",
    "            estimator.set_params(random_state=1)\n",
    "\n",
    "        estimator.train(X, y)\n",
    "\n",
    "        y_pred = estimator.predict(X)\n",
    "        incorrect = y_pred != y\n",
    "        estimator_error = np.dot(incorrect, sample_weight) / np.sum(sample_weight, axis=0)\n",
    "\n",
    "        # if worse than random guess, stop boosting\n",
    "        # if estimator_error.all() >= 1.0 - 1 / self.n_classes_:\n",
    "        #     return None, None, None\n",
    "\n",
    "        y_predict_proba = estimator.predict_proba(X)\n",
    "        # repalce zero\n",
    "        y_predict_proba[y_predict_proba < np.finfo(y_predict_proba.dtype).eps] = np.finfo(y_predict_proba.dtype).eps\n",
    "\n",
    "        y_codes = np.array([-1. / (self.n_classes_ - 1), 1.])\n",
    "        y_coding = y_codes.take(self.classes_ == y[:, np.newaxis])\n",
    "\n",
    "        # for sample weight update\n",
    "        intermediate_variable = (-1. * self.learning_rate_ * (((self.n_classes_ - 1) / self.n_classes_) *\n",
    "                                                              inner1d(y_coding, np.log(\n",
    "                                                                  y_predict_proba))))  #dot iterate for each row\n",
    "\n",
    "        # update sample weight\n",
    "        sample_weight *= np.exp(intermediate_variable)\n",
    "\n",
    "        sample_weight_sum = np.sum(sample_weight, axis=0)\n",
    "        if sample_weight_sum <= 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # normalize sample weight\n",
    "        sample_weight /= sample_weight_sum\n",
    "\n",
    "        # append the estimator\n",
    "        self.estimators_.append(estimator)\n",
    "\n",
    "        return sample_weight, 1, estimator_error\n",
    "\n",
    "\n",
    "    def discrete_boost(self, X, y, sample_weight):\n",
    "        estimator = deepcopy(self.base_estimator_)\n",
    "        if self.random_state_:\n",
    "            estimator.set_params(random_state=1)\n",
    "\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        y_pred = estimator.predict(X)\n",
    "        incorrect = y_pred != y\n",
    "        estimator_error = np.dot(incorrect, sample_weight) / np.sum(sample_weight, axis=0)\n",
    "\n",
    "        # if worse than random guess, stop boosting\n",
    "        if estimator_error >= 1 - 1 / self.n_classes_:\n",
    "            return None, None, None\n",
    "\n",
    "        # update estimator_weight\n",
    "        estimator_weight = self.learning_rate_ * np.log((1 - estimator_error) / estimator_error) + np.log(\n",
    "            self.n_classes_ - 1)\n",
    "\n",
    "        if estimator_weight <= 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # update sample weight\n",
    "        sample_weight *= np.exp(estimator_weight * incorrect)\n",
    "\n",
    "        sample_weight_sum = np.sum(sample_weight, axis=0)\n",
    "        if sample_weight_sum <= 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # normalize sample weight\n",
    "        sample_weight /= sample_weight_sum\n",
    "\n",
    "        # append the estimator\n",
    "        self.estimators_.append(estimator)\n",
    "\n",
    "        return sample_weight, estimator_weight, estimator_error\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "        pred = None\n",
    "\n",
    "        if self.algorithm_ == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            pred = sum(self._samme_proba(estimator, n_classes, X) for estimator in self.estimators_)\n",
    "        else:  # self.algorithm == \"SAMME\"\n",
    "            pred = sum((estimator.predict(X) == classes).T * w\n",
    "                       for estimator, w in zip(self.estimators_,\n",
    "                                               self.estimator_weights_))\n",
    "\n",
    "        pred /= self.estimator_weights_.sum()\n",
    "        if n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            pred = pred.sum(axis=1)\n",
    "            return self.classes_.take(pred > 0, axis=0)\n",
    "\n",
    "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.algorithm_ == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            proba = sum(self._samme_proba(estimator, self.n_classes_, X)\n",
    "                        for estimator in self.estimators_)\n",
    "        else:  # self.algorithm == \"SAMME\"\n",
    "            proba = sum(estimator.predict_proba(X) * w\n",
    "                        for estimator, w in zip(self.estimators_,\n",
    "                                                self.estimator_weights_))\n",
    "\n",
    "        proba /= self.estimator_weights_.sum()\n",
    "        proba = np.exp((1. / (self.n_classes - 1)) * proba)\n",
    "        normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "        normalizer[normalizer == 0.0] = 1.0\n",
    "        proba /= normalizer\n",
    "\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AdaBoost class\n",
    "def compute_error(y, y_pred, w_i):\n",
    "    '''\n",
    "    Calculate the error rate of a weak classifier m. Arguments:\n",
    "    y: actual target value\n",
    "    y_pred: predicted value by weak classifier\n",
    "    w_i: individual weights for each case\n",
    "    \n",
    "    Note that all arrays should be the same length\n",
    "    '''\n",
    "    return (sum(w_i * (np.not_equal(y, y_pred)).astype(int)))/sum(w_i)\n",
    "\n",
    "def compute_alpha(error):\n",
    "    '''\n",
    "    Calculate the weight of a weak classifier m in the majority vote of the final classifier. This is called\n",
    "    alpha in chapter 10.1 of The Elements of Statistical Learning. Arguments:\n",
    "    error: error rate from weak classifier m\n",
    "    '''\n",
    "    return np.log((1 - error) / error)\n",
    "\n",
    "def update_weights(w_i, alpha, y, y_pred):\n",
    "    ''' \n",
    "    Update individual weights w_i after a boosting iteration. Arguments:\n",
    "    w_i: individual weights for each observation\n",
    "    y: actual target value\n",
    "    y_pred: predicted value by weak classifier  \n",
    "    alpha: weight of weak classifier used to estimate y_pred\n",
    "    '''  \n",
    "    return w_i * np.exp(alpha * (np.not_equal(y, y_pred)).astype(int))\n",
    "\n",
    "class Ada:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, base_estimator=DecisionTree(max_depth = 3)):\n",
    "        '''\n",
    "        Ada constructor\n",
    "        base_estimator: \n",
    "        '''\n",
    "        self.base_estimator = base_estimator\n",
    "        self.alphas = []\n",
    "        self.G_M = []\n",
    "        self.M = None\n",
    "        self.training_errors = []\n",
    "        self.prediction_errors = []\n",
    "\n",
    "    def fit(self, X, y, M = 100):\n",
    "        '''\n",
    "        Fit model. Arguments:\n",
    "        X: independent variables - array-like matrix\n",
    "        y: target variable - array-like vector\n",
    "        '''\n",
    "        \n",
    "        # Clear before calling\n",
    "        self.alphas = [] \n",
    "        self.training_errors = []\n",
    "        self.M = M\n",
    "\n",
    "        # Iterate over M weak classifiers\n",
    "        for m in range(0, M):\n",
    "            \n",
    "            # Set weights for current boosting iteration\n",
    "            if m == 0:\n",
    "                w_i = np.ones(len(y)) * 1 / len(y)  # At m = 0, weights are all the same and equal to 1 / N\n",
    "            else:\n",
    "                # (d) Update w_i\n",
    "                w_i = update_weights(w_i, alpha_m, y, y_pred)\n",
    "            \n",
    "            # (a) Fit weak classifier and predict labels\n",
    "            G_m = self.base_estimator   # Stump: Two terminal-node classification tree\n",
    "            G_m.fit(X, y, sample_weight = w_i)\n",
    "            y_pred = G_m.predict(X)\n",
    "            \n",
    "            self.G_M.append(G_m) # Save to list of weak classifiers\n",
    "\n",
    "            # (b) Compute error\n",
    "            error_m = compute_error(y, y_pred, w_i)\n",
    "            self.training_errors.append(error_m)\n",
    "\n",
    "            # (c) Compute alpha\n",
    "            alpha_m = compute_alpha(error_m)\n",
    "            self.alphas.append(alpha_m)\n",
    "\n",
    "        assert len(self.G_M) == len(self.alphas)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict using fitted model. Arguments:\n",
    "        X: independent variables - array-like\n",
    "        '''\n",
    "\n",
    "        # Initialise dataframe with weak predictions for each observation\n",
    "        weak_preds = pd.DataFrame(index = range(len(X)), columns = range(self.M)) \n",
    "\n",
    "        # Predict class label for each weak classifier, weighted by alpha_m\n",
    "        for m in range(self.M):\n",
    "            y_pred_m = []\n",
    "            pred = self.G_M[m].predict(X)\n",
    "            for i in range(len(pred)):\n",
    "                y_pred_m.append(pred[i]*self.alphas[m])\n",
    "            weak_preds.iloc[:,m] = y_pred_m\n",
    "\n",
    "        # Calculate final predictions\n",
    "        y_pred = (1 * np.sign(weak_preds.T.sum())).astype(int)\n",
    "\n",
    "        return y_pred, weak_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression prediction of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Accuray at 0 iterations is 0.4154929577464789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LogisticRegression at 0x1adef3ceb50>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(epochs=5000)\n",
    "lr.train(x_train,y_train, lr=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the probability of each case into the final prediction of classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_pointer(y_cat, y_pred):\n",
    "    \"\"\"\n",
    "    This function translate numerical training result to categorical.\n",
    "    It also summarize which class the prediction indicated.\n",
    "    \"\"\"\n",
    "    ind = y_pred.argmax(axis=1)\n",
    "    y_class = []\n",
    "    y_pointer = list(enumerate(np.unique(y_cat)))\n",
    "    for i in range(len(ind)):\n",
    "        y_class.append(y_pointer[ind[i]][1])\n",
    "    return y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_train = lr.predict(np.insert(x_train, 0, 1, axis=1))\n",
    "y_pred_train = lr.predict(x_train)\n",
    "y_pred_train = category_pointer(y_cat=y_train,y_pred=y_pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following confusion matrix indicated that some of the cases are not correctly classified. The accuracy of the model is 82.4%. As for class=3, it did not predict correctly at all. The reason of the misclassification can be the uneven dataset split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[52,  7,  0],\n",
       "       [ 7, 64,  0],\n",
       "       [ 3,  9,  0]], dtype=int64)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=y_train, y_pred=y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8239"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    return round(np.sum(y_pred==y_true)/len(y_true),4)\n",
    "\n",
    "accuracy(y_pred=y_pred_train, y_true=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit LDA and Project Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (142, 13)\n",
      "Shape of transformed X: (142, 2)\n"
     ]
    }
   ],
   "source": [
    "# Project the data onto the 2 primary linear discriminants\n",
    "ldaModel = LinearDiscriminantAnalysis(2)\n",
    "ldaModel.fit(x_train, y_train)\n",
    "x_projected = ldaModel.transform(x_train)\n",
    "\n",
    "print(\"Shape of X:\", x_train.shape)\n",
    "print(\"Shape of transformed X:\", x_projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization illustrated the 3 classes of cases distributed in the 2 major discriminant component. When the linear discriminant 1 is less than -8, it would probably be the same group. Otherwise, linear component 2 = 1.5 can be a divider for the other 2 groups.\n",
    "\n",
    "This division figure looks effective and clean. No overlap or misclassification cases appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\youyu\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\collections.py:206: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  offsets = np.asanyarray(offsets, float)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1, x2 = x_projected[:, 0], x_projected[:, 1]\n",
    "\n",
    "plt.scatter(\n",
    "    x_projected[:, 0], x_projected[:, 1], c=y_train, edgecolor=\"none\", alpha=0.8, cmap=plt.cm.get_cmap(\"viridis\", 3)\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Linear Discriminant 1\")\n",
    "plt.ylabel(\"Linear Discriminant 2\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree prediction of training dataset reach 100% correct. The reason that it makes 100% correct is because decision tree has already classify each case by the conditions. If the max_depth is not too small, it will definitely classify each case into correct value. \n",
    "\n",
    "The cons is, such classification may not work as well in testing dataset as it used for training dataset. Since the training dataset and fitting process recorded too much (maybe not necessary) information about how to split the data and build the tree, overfitting can be a serious issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[59,  0,  0],\n",
       "       [ 0, 71,  0],\n",
       "       [ 0,  0, 12]], dtype=int64)"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtclf = DecisionTree(min_samples_split=2, max_depth=4)\n",
    "dtclf.fit(x_train,y_train)\n",
    "y_pred_dtclf = dtclf.predict(x_train)\n",
    "\n",
    "confusion_matrix(y_true=y_train, y_pred=y_pred_dtclf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost Training & Prediction\n",
    "\n",
    "Use Decision Tree with max depth 2 as a base estimator. Set the default of boosting time as 3. \n",
    "\n",
    "The results shows that the accuracy is 41.55% which is not ideal for prediction. class=1 cases are all correctly predicted, but the other 2 types are not correctly predicted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AdaBoostClassifier at 0x1adef37a4f0>"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaclf = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTree(max_depth=3),\n",
    "    n_estimators = 10,\n",
    "    learning_rate = 0.001,\n",
    "    algorithm = 'SAMME'\n",
    "    )\n",
    "\n",
    "initial_weight = [1/len(x_train)]*len(x_train)\n",
    "adaclf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57,  2,  0],\n",
       "       [ 0, 71,  0],\n",
       "       [ 0,  0, 12]], dtype=int64)"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_adaclf = adaclf.predict(x_train)\n",
    "\n",
    "confusion_matrix(y_true=y_train, y_pred=y_pred_adaclf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = Ada(base_estimator=DecisionTree(min_samples_split=3,max_depth=2))\n",
    "ab.fit(x_train, y_train, M=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[59,  0,  0],\n",
       "       [71,  0,  0],\n",
       "       [12,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ab, weak_preds = ab.predict(x_train)\n",
    "\n",
    "print(accuracy(y_pred=y_pred_ab, y_true=y_train)*100)\n",
    "\n",
    "confusion_matrix(y_true=y_train, y_pred=y_pred_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Predict the models using the test dataset, and provide the performance metrics. Compare the five models' performance metrics, and explain at least four findings on each of the models. Do not need to repeat the code). (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the accuracy and confusion matrix, the performance on logistic regression did not predict correctly at all. \n",
    "\n",
    "The reason can be the Y differences between training and testing data. \n",
    "\n",
    "The training data contains a lot of class=1 and class=2. However, the testing data are all class=3. Such difference will degrade the training weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test_lr = lr.predict(np.insert(x_test, 0, 1, axis=1))\n",
    "y_pred_test_lr = category_pointer(y_cat=y_train,y_pred=y_pred_test_lr)\n",
    "\n",
    "accuracy(y_pred=y_pred_test_lr, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 2, 34,  0]], dtype=int64)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=y_test, y_pred=y_pred_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA project could not devide the categories in testing data, because the testing data only have class=3, and thus LDA could not separate classes as far as possible. \n",
    "\n",
    "In this graph, the only category in the test dataset is class=3, and we don't have any other group to separated from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (36, 2)\n",
      "Shape of transformed X: (36, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\youyu\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\collections.py:206: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  offsets = np.asanyarray(offsets, float)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEKCAYAAADkYmWmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAx7klEQVR4nO3deXxV1bXA8d+6mUggECABAgHCKDIPQXFCxAm1Dm3t62C19lnp4GtrtZMdtb7XV1+H177aCYdXa+1rrdrWoVonVNSCDDIGZAxjgAQISUjIuN4f5yRckpt7T5J77pCs7+dzP+Tuu+85K+Gycthn77VFVTHGGJOcAvEOwBhjTNdZEjfGmCRmSdwYY5KYJXFjjElilsSNMSaJWRI3xpgk5lsSF5E+IvKOiKwTkU0ick8H/f5FRIrdPn/wKx5jjIknLzlRRD4jIhtEZK2IvCkikyMe16954iIiQF9VrRaRNOBN4IuqujyozwTgcWChqh4TkSGqetiXgIwxJo485sT+qlrpfn0N8DlVXRTuuKl+BazOb4dq92ma+2j7G+NW4Beqesx9jyVwY0yP5CUntiRwV9+2r4fiWxIHEJEUYDUwHidZr2jTZaLb7y0gBbhbVV8IcZzFwGKAvn37zpk0aZKfYRtjeojVq1eXq2ped45x+cKheuRovbfzravYBJwMalqiqktannjIiYjIbcAdQDqwMNI5fRtOaRNUDvAX4POqujGo/VmgAfgXoAB4A5imqhUdHauoqEhXrVrla7zGmJ5BRFaralF3jlE0c6C+8/JFnvqm5P3F0/k6yolt+nwMuFxVPxHuWDGZneIm5aVA27GdfcDTqtqgqruArcCEWMRkjDHxEiYnBvsjcF2kY/k5OyXP/W2DiGQClwJb2nT7K7DA7ZOLM7yy06+YjDEmXrzkRHeyR4urgG2RjuvnmHg+8Ig7BhQAHlfVZ0Xke8AqVX0a+AdwmYgUA03AV1T1iI8xGWNMvHjJif8mIpfgDDMfA8IOpYC/s1PWA7NCtH8n6GvFGcC/w684jDEmEXjMiV/s7HFtxaYxxiQxX6cYGpMsDlVX85ctxeyrqmTS4FyumzSZfunp8Q7LmIgsiZter+JkLV956QUq6pzpvesPHWR16QF+fNkVBETiHJ0x4dlwiun1Xt65ozWBt9hx7CjvHjwQp4iM8c6SuOn1jp2sDd1eG7rdmERiSdz0enPyR7RrSxFh5rDhcYjGmM6xJG56vdn5w7n+zCmkivPPITM1lc+fdQ65WVlxjsyYyOzGpjHATTNmcc0ZkyitrqZwQA6ZaWnxDskYTyyJG+PK6ZNJTp/MeIdhTKfYcIoxxiQxS+LGGJPELIkbY0wSsyRujDFJzJK4McYkMUvixhiTxPzc2aePiLwjIutEZJOI3BOm7wdFREWkW3vhGWNMb+PnPPE6YKGqVotIGvCmiDyvqsuDO4lINvBFoN2uz8YYY8Lz7UpcHdXu0zT3oSG63gvcB5wM8ZoxxpgwfB0TF5EUEVkLHAZeUtUVbV6fDYxU1ef8jMMYY3oqX5O4qjap6kygADhLRKa2vCYiAeAnwJ2RjiMii0VklYisKisr8y1eY4xJNjGZnaKqFcBSYFFQczYwFXhNREqAecDToW5uquoSVS1S1aK8vLwYRGyMMcnBz9kpeSKS436dCVwKbGl5XVWPq2quqhaqaiGwHLhGVVf5FZMxxvQ0fl6J5wNLRWQ9sBJnTPxZEfmeiFzj43mNMabX8G2KoaquB2aFaP9OB/0X+BWLMcb0VLZi0xhjkpglcWOMSWKWxI0xJolZEjcmyagq7x0pZ0t5GaqhFkGbROSlnpSI3CEixSKyXkReEZHRkY5re2wak0QOn6jme68vZU/lcQBG9h/Ad+ZfxNB+/eIcmfHASz2pd4EiVa0Rkc8C/wV8ONxB7UrcmCTywJpVrQkcYG/lcX6z+p04RmS88lJPSlWXqmqN+3Q5zmr3sCyJG5NE3i090K5tzcHSOERiuiJSPak2bgGej3RMS+LGJJHcrL7t2vKysuIQielAbkudJ/exOPjFcPWkgonIx4Ei4IeRTmhJ3Jgk8qEp7f/Nf2hyyDxg4qO8pc6T+1gSqlMH9aQAEJFLgG/ilCGpi3RCu7FpTBK5eMw4BvXJ5OVdOwBYOGYsc/JHxDkq44WI5AENqloRVE/qvjZ9ZgG/ARap6mEvx7UkbkySmZU/nFn5w+Mdhum8fOAREUnBGQV5vKWeFLBKVZ/GGT7pB/xZRAD2qGrYWlOWxI0xJga81JNS1Us6e1wbEzfGmCRmSdwYY5KYJXFjjElilsSNMSaJ+bk9my/FXowxxpzi55V4S7GXGcBMYJGIzGvTp6XYy3TgCZxiL8YYYzzyLYn7VezFGGPMKb6OiUer2IuILG6pRVBWVuZDpMYYk5x8TeLRKvaiqktaahHk5eX5Fq8xxiSbmMxOiWaxF2OMMaf4OTslT0Ry3K9bir1sadOnpdjLNV6LvRhjjDnFz9opvhR7McYYc4pvSdyvYi+m59hfVck7+/bRLz2d80eNJjMtLd4hGZN0rIqhiYtXdu3g5+8sp9ndrf0PG9fzg4svsw1/jekkW3ZvYq6+qYmH313TmsAbm5o4WF3F48Ub4hyZMcnHkriJubITJ6iqr6OpuZn9lZWUHK9g9/EKnizexLHa2niHZ0xSCZvERWSSiFwsIv3atLebKmiMV3l9+5Kdnk55zQlqGxta2082NfKbNSvjGJkxyafDJC4iXwD+Bnwe2Cgi1wa9/H2/AzM9V3pKCjfPmM2JhlMJPDUQYGCfTN7Zty+OkRmTfMLd2LwVmKOq1SJSCDwhIoWq+jNAYhKd6bEuHTee6UOGsbfyOAER+qWnkxII0D8jI96hGZNUwg2nBFoKWKlqCbAAuEJEfoIlcRMFH5s2nYGZmQzo04eUgPNRvG7S5DhHZUxyCXclfkhEZqrqWgD3ivx9wMPAtFgEZ3q2902cRL/0DF7ZtQMFFhaOZeGYsfEOy5ikEi6J3wQ0BjeoaiNwk4j8xteoTK+xoHAMCwrHxDsMY5JWh0lcVTu8w6Sqb/kTjjHGmM6weeLGGJPEbNl9L1NZV8fz295jT+VxJgzKZdH48fRJtZolxiSriFfiInKflzaT+GobGvjqyy/w2Mb1LNtdwsNrV/Ptpa+0Ln83xiQfL1filwJfa9N2RYg200WqyuObNvDMtvc42dDAuSNHc+vsIrKjPGd6acku9lRUcLjmBLUNDaRIgIqTtawu3c/c4ba9aW9TWXeSxzdtZHN5GSOy+3P95CmMGpAT77BMJ3WYxEXks8DngLEisj7opWzAbmxG0TNbt/DYxlM/4td27+JEQz3fnn9RVM9z6EQVB6qrqG9qAqBRmymvqWHZ7hJL4r1MsyrfWvoyJRUVAGw7eoSVB/bxP4veR17fvvENznRKuOGUPwBXA0+7f7Y85qjqxyMdWET6iMg7IrJORDaJyD0h+mSIyJ9EZLuIrHBXhvY6r5bsbNe26sB+KutORvU8uZlZrQk82KHqE1E9j0l8aw+WtibwFicaGnhp5/b4BNQLeMyJ80VkjYg0isj1Xo7bYRJX1eOqWqKqHwX2AQ2AAv1EZJSHY9cBC1V1BjATWCQi89r0uQU4pqrjgf8GeuVYu4RYAOvsdBTdhbFThw5jQEaf1qMKMDgzi4GZfaJ6HpP4KutCb2dbVW/b3PrIS07cA9yMcxHtScQxcRH5N+Bu4BDQ7DYrMD3c+1RVgWr3aZr7aHsH7Vr32ABPAPeLiLjv7TUuGTOOHceOntZ21vARUa8jMiZnIOeOHEVx2WHqm5rISE0lNRDg0nHjo3oek/hm5+eTHghQ39x8WvvZI0bGKaKez0tOdEucICKn/8WE4eXG5u3AGap6xOtBW7j7a64GxgO/UNUVbbqMAPaCsxpURI4Dg4HyNsdZDCwGGDXKy38CksuVEyZysqmRZ7e+R01DPeeNHM0ts+b4cq5vXbCAh9euZvWBAwzKzOSDk6cwJ3+EL+eKp6bmZjaXl5GRksqEwYPjHU7C6Z/Rhy+fewG/XLmCirqTZKSkcP3kqcwclh/v0BLO9sqBXPfyBzz2/kuuiKwKaliiqktannjIiZ3mJYnvBY535eCq2gTMdHe9/4uITFXVjV04zhJgCUBRUVGPu0oXET545hQ+eOYU3881MDOTO8853/fzxNO2I0f4/PPPUFpdjQicMTiXB66+jv4ZNmwUbF7BSObkD6e0uorcrL5k2R6n0VCuqkUdvRitnBjMy4rNncBrInKXiNzR8ujMSVS1AlgKtN1MYj8wEkBEUoEBQKev+I0J9qV/PMeeyuM0NDdR39TEhsOH+MarL8U7rISUlpLCqAE5lsBjLExO7DQvSXwP8BKQjjO9sOURlojkub9tEJFMnPnmW9p0exr4hPv19cCrvW083ETX0doaSo5XtGt/Z79tNmHiy2NO7LSIwymq2m4ajEf5wCPuGFAAeFxVnxWR7wGrVPVp4CHgURHZDhwFPtLFc5kks7uigr9vf4/jdXWcPaKABaPHuDNyuic9JTXknJ7UgJUJMnEXMSeKyFzgL8BA4GoRuUdVw46zepmdkgd8FZgCtA4qqurCcO9T1fXArBDt3wn6+iTwoUgxmJ5l+9Ej3PXKi9S5c9bf3ruHHUeP8qnZHQ4letYvPZ2i4SNYEXTlLcCicRO7fWxjusNjTlwJdGrlnZfLk8dwLvnHAPcAJYDtZmu67KnNxa0JvMXft22N2uKm/1n0PhYWjiE7PYOcjD5cP3kKXz9/flSObUyi8TI7ZbCqPiQiX1TV14HXRcSSuOmyspr2K0QbtZmjtbVRmUGSnZHB/VdeQ21DAwERMlKtWKcfWm5fRWMYzHSdl093y5bkpSJyFXAAGORfSKanmzksn/eOnLYUgMGZWVEvvpRpMy58Ud/UxEPvruLVXU65iIsKx3LLrDn2yzJOvPzU/11EBgB3Aj8H+gNf8jUq06N9YNJktpSXse7QQQCy09P50rxzCdgVXVL47do1PL99W+vzF3ZsIyUgfHrOWXGMqvfyMjvlWffL40B0y+qZXikzLY17L7qEnceOUllXx5m5eXYVl0SWhijY9uqunZbE48Tr7JRbgcLg/qr6r/6FZXqDsQO7PypX29DA6tIDpKekMGtYPmkpKVGIzIQT6n9MKWJTOOPFy+XP34BlwMtA+zqmxsTJ5vIy/v2NpVTV1wMwtG9f7r3oEob1i7gWzXTDpWPH89SW4tPaLhk7Lk7RGC9JPEtVbRcfk3B+tXJFawIHOHTiBL9d9y5fP8+mE/rp49NnEhDh5Z07ALh47FhumDYzvkH1Yl6S+LMicqWq/t33aIzxqKahIeTy+uLDh2MfTC+TGghw04xZ3DSj3boVEwdeBrK+iJPIa0WkUkSqRKTS78CMCadPaiqDMzPbtRf07x+HaIyJn4hJXFWzVTWgqpmq2t99bv9STFwFRLhh2ozT6qSkBwJ8dNqMuMVkTDyE2yh5kqpuEZHZoV5X1TX+hWVMZJeMHU9B/wEs21NCWiCFS8aOo6D/gHiHZUxMhRsTvwNnN50fh3hNgbAFsIyJhUm5eUzKzYt3GMbETYdJXFUXu3/aAh9jjElQXhb7pABX0X6xz0/8C8sYY4wXXmanPAPcjLOBcWd29hkpIktFpFhENonIF0P0GSAiz4jIOrfPJzsZvzHG9Gpe5okXqOr0Lhy7EbhTVdeISDawWkReUtXgpV63AcWqerW7vP89EXlMVetDHtEYY8xpvCTx50XkMlV9sTMHVtVSoNT9ukpENgMjgOAkrkC2OAWJ++Fs0dbYmfN0pLKujhe2b6W0uoopeUNYUDjWtugyxvQ4XpL4cuAvIhLAqS0ugHZmrriIFOJsS7SizUv342yWfABniObDqtoc4v2LcWbKMGrUqIjnq6yr484X/86hE87mA6/s2snyffv41vwFXkPutRqbm9l4+BABEaYOGdqjy8P+Y8c2XtyxncbmZhaMHsO1k87s0d+v6Zm8JPGfAOcAG7qyE72I9AOeBG5X1bYrPS8H1uJMVxwHvCQiy9r2U9UlwBKAoqKiiDG8tHN7awJv8c6BfWw9Us7Ewbmd/RZ6jd0VFdzzxquU19QAMDw7m+8tuJghffvFObLoe2brFh5Ys6r1+a6KY1TWn+QTM0IuizAmYXkZX9gLbOxiAk/DSeCPqepTIbp8EnhKHduBXcCkzp6nrdKqqpDtBzpoN45frVrRmsDB+Xk9/O7qOEbkn+e2vteu7e/bttLc+Y+5MXHl5Up8J/CaiDwP1LU0Rppi6I5zPwRsDtN3D3AxsExEhgJnuOfrlilDhvDizu2ntQVEmJI3pLuH7rEampooLi9r177+0KE4ROO/moaGdm11TU00NTcTsJrkJol4uRLfBbwCpNOJKYbAecCNwEIRWes+rhSRz4jIZ9w+9wLnisgG9xxfU9Xyjg7o1YWjx3DeyFNj5wERbp4xm7y+fbt76B4rLSWFwZlZ7dqH9OuZP7PzQtxbOWv4CNtUwiQdL9uz3dOVA6vqm0DYu0SqegC4rCvHDycgwtfOm8/OY0fZX1nJmXlDyM1qn6DM6T46dRr3rzx17zkgwkemdGV2aeK7afosjp+s4+19e2hWZfawfG6bOy/eYRnTaeEKYP1UVW8XkWdwpgKeRlWv8TWyKBg7cFBUtgDrLS4bN4Gh/bL565bi1hkqr+zaQX6/bEbn5MQ7vKjKTEvjq+ddQFVdHc2qDOjTJ94hGdMl4a7EH3X//FEsAjGJYUzOQLYeKaeuydmJb8X+fRSXlfGrq66hf0ZGnKMLr7G5mU2HD5GeksKk3DzEw3TB7AT/noyJJFwBrNVu3ZTFqnpDDGMycfTmnpLTtjwDqKqv4809JVw54Qxfz72r4hh/2riePcePc0ZuLh+dOt3z9Madx45y7xuvcaTWmV1TmJPD3RcuZFCIcf6Ozv16yS4CIiwoHMOoATld/TaMiamwY+Kq2iQio0Uk3ZbC9w51jaH3wu6oPVqO1NTwjVde5IQ7a2RfVSUbDh3il1ddQ7qHm42/WLmiNYEDlFRU8Mi6d/nSvPMivvefe/dw39vLWqcX/nXLZr45/0Lm5I/o4ndjTHsi0gd4A8jAyb1PqOp32/TJAH4HzAGO4CyALAl3XC+zU3YCb4nIt0XkjpZHF74HkwTOGTmSlDbDEKkS4JyRI30972u7d7Um8BaHa06wYv/eiO+taWhg29Ej7drXHTzo6dy/W7/2tPnhjdrM79ev8/ReYzqhDlioqjOAmcAiEWl7N/0W4Jiqjgf+G7gv0kG9JPEdwLNu385MMTRJaFi/bL5y7gXkZTlTC/Oy+vLlc89nWD9//8pP1If+j16o+dxt9UlNDTle72V6pKqyv6r9lrH7Ko9HfK8xneEuaqx2n6a5j7aTRq4FHnG/fgK4WCLc3PFtiqFJXueOHMW8gpFU1dWRnZERk3oi5xSM4snNm077RKcHAswdHnlIIyDCv0yeyoNBq0udtmkR3ysiTMrNZUv56csTbLcg00W5IrIq6PkSt2wI0Lo/w2pgPPALVW1bT2oEzip5VLVRRI7jlAHvcP2Ml00hXgI+pKoV7vOBwB9V9XJP35JJSgGRmE67mzB4MJ8tOptH179LVX09gzOz+MycuZ5vTF5zxpkMz+7fut/mZePGe66Tc+usudz9+iutN3RzMvpwy6w5Xf5eTK9WrqpFHb2oqk3ATBHJwSksOFVVN3bnhF6W3ee1JHA3iGMiYuvXTdQtGj+BhWPGUnGyltysvp3+H0DR8BEUebhyb2vC4ME8ePX7WXlgPyJw1vACMlK9/NMwpmtUtUJElgKLgOAkvh8YCewTkVRgAM4Nzg55+aQ2icgoVd0DICKjCbH4x5hoSE9J6VbVxPKaGl7auZ3KupOcNbyAWfnDPb0vMy2N+aMLu3xeYyJxN75pcBN4JnAp7W9cPg18AvgncD3waqTig16S+DeBN0XkdZxl9Bfg1vY2JpHsqzzO117+R+uwyHPbtvLRqdP56NSeWTrAJJ184BF3XDwAPK6qz4rI94BVqvo0TtHAR0VkO84mOR+JdFAvNzZfEJHZQMtUmNujUaTKmGh7onhTu4VKTxZv5OqJk+iXnh6nqIxxqOp6nM1x2rZ/J+jrk8CHOnPciFMMReQ8oFZVnwVygG+4QyrGJJRQUwXrm5s5fKI6RG9jegYv88R/BdSIyAzgDpx547/zNSpjuuDMENMC+2dkUNB/QByiMSY2vIyJN6qqisi1OPMaHxKRW/wOzCSn/VWVPLp+Le+VlzOyf38+Nm1GzOZcXz95KmsPlVJSUQE488w/V3S2p2X7xiQrL0m8SkTuAj4OzHc3TE6L9CYRGYlzxT4UZzbLElX9WYh+C4CfuscsV9ULvQZvEktdYyPfevUljtTWAnCktobN5WXcf8XVDO3n/z6d/TMy+OnlV7H2YCnH604ya1g+OX0yfT+vMfHkZTjlwzhr/m9R1YNAAfBDD+9rBO5U1ck4N0VvE5HJwR3cCe+/BK5R1Sl0ckDfJJZ39u9rTeAt6pqaWFrS7R33PAuIMDt/OBcVjrUEbnoFL7NTDuLseN/yfA8exsRVtRQodb+uEpHNOEtKi4O6fQxno+Q9br/DnYreJJT6ptCVDjtqN8Z0X4dX4iLypvtnlYhUBj2qRKT9NIAwRKQQZ2pN2zoBE4GBIvKaiKwWkZs6Gb9JIHNHFNCnzUrHgAjnj7LJTMb4JdymEOe7f3arfJ2I9AOexJlf3jb5p+LUzb0YyAT+KSLLVXVrm2Msxl1gNCrEBrcmMfTPyOCb51/Ir1evZH9VJYMzs7hpxkzbIq8XWnewlD9t2sDBE9VMGzKUG6fPsn1ufeKlANY0YJL7tFhVN3k9uIik4STwx1T1qRBd9gFHVPUEcEJE3gBmAKclcbcK2BKAoqIiW/KfwGYMy+eXV15NdX09fdPTY1IB0SSWnceOcs/rS2nUZgCWluxix9Gj/M8V7+vy56HshFNbPjMtjXMLRpGZFnFuRa8RbqPkAcDfgFHAOpwl99NEZA9wbYir6rbvF5wlpJtV9ScddPsbcL9b6CUdOBunELpJYiJie1f2Yi/t3N6awFvsqTzOxsOHmD50WKeP9/bePfzo7Tdbj/n79Wv5/sLLyM+2bQ0g/OyUe4FVwHhVfb+qXgdMAFYC/+Hh2OcBNwILRWSt+7hSRD4jIp8BUNXNwAvAeuAd4MHulmU0xsRXh1v8NTV2+lhNzc08sGblab8UjtTW8sdN67scX08TbjjlEmC66qmfnqo2i8g3gA2RDqyqb+JcvUfq90O8TVk0xiSB+aMLeXnXjtPaBmRkMGNofqePdfRkbbtpqwDvHbHyTS3CXYnXq2q7X51uW51/IRljktnMYfncOruI7HRnSK0wJ4dvz7+oSytnB/bJJCej/eYkY3IGdjvOniLclXgfEZlF+6tpwdmt2RhjQrp64iSuGD+RmoaGkPufepUaCPCJGbP4+crlrZtZZ6enW3nhIOGSeClBi3za8LaNuDEmKnYeO8q6gwcZ0q8vZ48YSWrAy2Lr+EoNBLqVwFtcPHYc4wcN5p/79pCZlsaC0WNiunVgogs3T/yiWAZijAntjxvX84eNp27kjR04kO8vvIysXjTNbnRODqNzcuIdRkJK/F/nxvRi5TU1/HHT6fMIdh47xgvbt3bwDtPb2G6wPdDbe/fw3Lb3qK6v59yRo/jgmVOS4r/fpr2dx462jgUH23Y07N65phcJm8TdBTsFqro3RvGYbnp77x5+8NYbrc93VRzjUHU1Xzj7nDhGZbpq9IAcAiLtErnNzjAtwl6eubss/z1GsZgoeHbrlnZtr5XsorrN3pMmOQzt14/rzjjztLaR/QdwxfiJcYrIJBovwylrRGSuqq70PRrTbdUN7ZN1ozZzsrHRNgtOUjfPnM1ZIwpYd+ggQ7L6cv6o0WSk2kiocXj5JJwN3CAiu4ETOPPEVVVtomYCOqdgVOv2ZC0mDhpsFeSS3OS8IUzOGxLvMEwC8pLEL/c9ChM1HzxzCoeqq3l99y6aVJkwaDB3nnN+vMMyxvjEy84+uwFEZAhgM+wTXHpKCrfPO5dbZs3hZFMjeVl94x1STNQ1NvK/a9fwWskuUgIBLh83nhumzSDFZuWYHs5LPfFrgB8Dw4HDwGhgMzDF39BMd2RnZJDdi6ojPLBmFS/u3N76/InNm0gJBLhh2ow4RmWM/7xcptyLs9HxVlUdg7MLz3JfozKmE5qam3ktxGbMr7SppGdMT+QliTeo6hEgICIBVV0KFPkclzHdFmKNjDE9jpcbmxXuPpnLgMdE5DDOLBVjumzZ7pLWK+WLx4zjgtGFXT5WSiDA/NFj2tWwvnjM2O6EaExUichI4HfAUECBJar6szZ9BgIPA+OAk8C/Rtoox0sSvxaoBW4HbgAGAN+LRsBBfecC/wQ+oqpPeIjJJLHntr7Hb9acWnaw5mApx+tO8r6Jk8K8K7xPz5lLWkqA10p2kRoIcNm48Vau1CSaRuBOVV0jItnAahF5SVWLg/p8A1irqu8XkUnAL3CGsDvkZXbKCREZDUxQ1UdEJAvwUt3dS8CISApwH/Cih2OaHuCpLcXt2p7cXNytJJ6Rmspni87ms0Vndyc0Y3yjqqU4Jb5R1SoR2QyMAIL/QUwGfuD22SIihSIyVFUPdXRcL7NTbgUWA4NwLvFHAL8mwm8HjwEDfB54EpgbKRbTMxw/2X67rcq6k3GIxJjI0g/VUPDjiDtStsgVkVVBz5eo6pK2nUSkEJgFrGjz0jrgA8AyETkLZzZgAdBhEvdyY/M2nE2PKwFUdRvQqaVjHQUsIiOA9wO/ivD+xSKySkRWlZWVdebUJgEVjSho1zZ3+Ig4RGJM1JWralHQI1QC74dz4Xq7qla2efkHQI6IrMW5wH0XCL3ztMvLmHidqtY7BQ1BRFJxxrg9iRDwT4GvuRswd3gM9wexBKCoqMjmHCS5T8+ey9HaGraUO5vdnjE4l8Vz7D9ipucTkTScfPiYqj7V9nU3R37S7SvALqD9/NkgXpL46+4O95kicinwOeCZaASMM1Xxj24CzwWuFJFGVf2rl+Ob5DQwM5P/umQR+ysrUZSC/gPiHZIxvnOT8kPAZlUNufWliOQANapaD3wKeCPExe9pvCTxrwO3ABuAT+OUpn0wGgG7i4da+v8WeNYSeO8xon//eIdgTCydB9wIbHCHS8CZjTIKQFV/DZwJPCIiCmzCyb1heZmd0gw84D6iHbAxxvQKqvomThXYcH3+CXSqWLyX2SnnAXfj3CVN5VQp2rArKbwE3Kb/zV77GmOMcXgZTnkI+BKwmgh3SY0xxsSWlyR+XFWf9z0SY4wxneYliS8VkR8CTwF1LY2qusa3qIwxxnjidXs2OL1yoQILox+OMcaYzvAyO+WiWARijDGm8zpM4iLycVX9vYjcEer1juZ+G2OMiZ1wV+ItmzNmh3jNlr4bYzxbtqeEF3dsp6m5mQsLx3D5uAnxDqnH6DCJq+pv3D/vafuaiNzuY0zGmB7kxR3buH/lqdp3G8sOc6y2lo9Yvfeo6OpW4CGHWIwxpq2/btncru2ZrVtotv3zoqKrSdzzSsxkc6K+nvomW9NkTLRU1te1a6uur6epuTkO0fQ8XqYYhtLjfoUeqq7mZyveZmPZYfqkpnLVhIncOH0WgTAlco0xkZ09ooCXdp6+/+mc/OGkpXjZIMxEEm52ShWhk7UAmb5FFCf3vfUG248dBeBkYyNPbi4mN7MvV008I86RGZPcbp4xm/KaGt49WAo49eM/N3denKPqOcLd2Aw1K6VHOlBV2ZrAgy3bU2JJ3Jhuys7I4J4FF1N24gSNzc3kZ/ea1BITXR1O6VHSU0L/GNLtv3vGRE1e376RO5lO6+qNzR4lNysr5B6PV4zvVFlfY4yJOd+SuIiMFJGlIlIsIptE5Ish+twgIutFZIOIvC0iM/yKJ5Ivn3M+V088gyFZfRk3cBB3zjuPc0aOilc4xhjjiZ/DKY3Anaq6RkSygdUi8pKqFgf12QVcqKrHROQKnM2Qzw51ML9lpqVx6+y53DrbNuw1xiQP35K4qpYCpe7XVSKyGRgBFAf1eTvoLcuBAr/iMcaYnigmY+IiUgjMAlaE6XYLEHLzCRFZLCKrRGRVWVmZDxEaY0xy8j2Ji0g/4EngdlWt7KDPRThJ/GuhXlfVJapapKpFeXl5/gVrjDFJxtcphiKShpPAH1PVpzroMx14ELhCVY/4GY8xxvQ0fs5OEZxNljd3VHtcREbhbPt2o6pu9SsWY4zpqfy8Ej8PuBHYICJr3bZvAKMAVPXXwHeAwcAvnZxPo6oWtT+UMcaYUPycnfImEaodquqngE/5FYMxxvR0tmLTGGOSmCVxY4xJYlYAy5gYW3lgH38u3siRmlpmDcvnxukzGdCnT7zDMj4TkZHA74ChOGW+l6jqz9r0GQD8HufeYSrwI1X933DHtSRuTAxtLjvMfyx7vXVrshd3bqek4hg/uuyKOEdmYsBLKZLbgGJVvVpE8oD3ROQxVa3v6KA2nGJMDL2wY1u7vSW3Hj3C9qO2RKKnU9VSVV3jfl0FtJQiOa0bkO1O0e4HHMVJ/h2yK3FjYqiug/1b6xrD/js1PUyYUiT3A08DB4Bs4MOqGnYzUrsSNyaG5o8qbNeWl9WXM/OGxD4Y44fcljpP7mNx2w4RSpFcDqwFhgMzgftFpH+4E9qVeAJZWrKTJ4s3cezkSYqGD+dfZ86xG149zLkjR3Hj9Jn8ZXMx1Q31jB84iM+ffY5tyN1zlIdbsOihFMkngR+oqgLbRWQXMAl4p6NjWhJPEO/s38d/Lz9VmXdpyS4OVVfzg0suj2NUxg8fmjyV6844k5ONjWRnZMQ7HBMjXkqRAHuAi4FlIjIUOAPYGe64lsQTxIs7trdrKy4vY+/x44wcMCAOEfnjSE0Nf3tvM3uOVzBhcC7XTJzUKxNZWkoKabaHa2/jpRTJvcBvRWQDzor3r6lqebiDWhJPEE0d3LtoDH9PI6lU19fz1Zf/QVnNCQDWHCxlxb69/OTyK0kN2O0Z07N5LEVyALisM8e1fzkJYsHoMe3aCnNyGJMzsNPH2nqknN+sfocH1qxkRwJNXXu9ZFdrAm9RcryClfv3xSkiY5KfXYkniAsLx3CktoYnNxdTXV/HjKHDuG3uvE4fZ9meEn78z7da5yI/t20rd503n7MLRkY75E47UlsTsv1obW2MIzGm57AknkA+cOYUrps0mcbmZtK7OF762IZ1py0maVblsQ3rEiKJz84fzhObN53WJsCs/Pz4BGRMD2DDKQkmINLlBA5QWlXVru1AVchd8WJu6pChfHjKNFLF+dhlpKSwePZchmeHnQZrjAnDtytxj8VeBPgZcCVQA9zcsizVdM2UvCFsLDt8WtvUIUPjFE17N0ybwVUTJrKvspLRA3J65cwUY6LJzyvxlmIvk4F5wG0iMrlNnyuACe5jMfArH+PxzZGaGtYeLKXiZPzHdhfPmcvAoAVCgzOzuGVWYm2WlNMnk6lDhloCNyYK/NzZpxQodb+uEpGWYi/BFbuuBX7nrk5aLiI5IpLvvjcp/H79Wp7cvIkmVdICAW6YNoMPnDklbvEU5gzkgavfz9qDzo9w5rD8bg3PGGMSW0zGxMMUexkB7A16vo/2Vb0QkcUttQjKysp8i7OzNpcd5vHijTS5NxIbmpv57bp32V1REde40lNSOGtEAWeNKLAEbkwP53sSj1DsxRNVXaKqRapalJeXF90Au+Hdg6H/w9BRuzHGRJuvSdxDsZf9QPDctwK3LSkMzsoK2Z7bQbsxxkSbb0ncY7GXp4GbxDEPOJ5M4+HzRxUyvF/2aW2FA3I4e0RBnCIyxvQ2fi728VLs5e840wu340wx/KSP8URdZloa911yOc9s3UJJRQXjBw3ifRMnWWEjY0zM+Dk7xUuxF8XZUy5pDejTh49PnxnvMIwxvZSt2DTGmCRmSdwYY5KYJXFjjElilsSNMSaJWRLvRRqbmzlRXx/vMIwxUWT1xHuJPxdv5K9biqmqr+eMwbl84axzetTencb0VnYl3gss213Co+vXUuVehb93pJx/X7b0tM0jjDHJyZJ4L/DGnpJ2baXV1Ww9EnYTbWNMErAk3gt0VMkwI9VG04xJdpbEe4FF4ye2Wzo7KTeXMTkD4xKPMSZ67FKsF5g2ZCh3nX8hT27eRHlNDbPz8/nEjFnxDssYEwWWxHuJeQUjmZcAO94bY6LLhlOMMSaJWRI3xpgkZkncGGOSmCVxY4yJAREZKSJLRaRYRDaJyBdD9PmKiKx1HxtFpElEBoU7riVxY4yJjUbgTlWdDMwDbhORycEdVPWHqjpTVWcCdwGvq+rRcAe1JG6MMTGgqqWqusb9ugrYDIwI85aPAv8X6biiSVY/Q0TKgN0xPm0ukGhr1BMxJkjMuBIxJkjMuBIxJuh6XKNVNa87JxaRF9zze9EHOBn0fImqLglxzELgDWCqqlaGeD0L2AeMj3QlnnTzxLv7F9IVIrJKVYtifd5wEjEmSMy4EjEmSMy4EjEmiG9cqroomscTkX7Ak8DtoRK462rgrUgJHGw4xRhjYkZE0nAS+GOq+lSYrh/Bw1AKWBI3xpiYEBEBHgI2q+pPwvQbAFwI/M3LcZNuOCVO2o1pJYBEjAkSM65EjAkSM65EjAkSN67OOA+4EdggImvdtm8AowBU9ddu2/uBF1X1hJeDJt2NTWOMMafYcIoxxiQxS+LGGJPELImHICJ/Clr6WhI0ftW23yIReU9EtovI12MQ1+dFZIu7ZPe/OujzJff1jSLyfyLSJ0HiyhGRJ9x+m0XknHjH5PZLEZF3ReRZP+PxGpeXpdmxjsntE+vP+t0isj/o3+GVHfSL+ec94aiqPcI8gB8D3wnRngLsAMYC6cA6YLKPcVwEvAxkuM+HhOgzAtgFZLrPHwdu9vnnEzEut/0R4FPu1+lATrxjcl+7A/gD8GwMPkte/g7zgdnu19nA1gT4XMX0s+6e827gyxH6xPzznogPuxIPw50S9C+Enq95FrBdVXeqaj3wR+BaH8P5LPADVa0DUNXDHfRLBTJFJBXIAg74GJOnuNwpU/NxplehqvWqWhHPmNy4CoCrgAd9jKVTcWnnl2b7HhOx/6x3Rqw/7wnHknh4FwCHVHVbiNdGAHuDnu/D339sE4ELRGSFiLwuInPbdlDV/cCPgD1AKXBcVV/0MSZPcQFjgDLgf92hiwdFpG+cYwL4KfBVoNnHWLoSF9C6NHsWsCLOMcX6s97i30RkvYg8LCLtNoSN0+c94fTaeeIi8jIwLMRL31TVlkn2ngrQxCImnL+rQTjVz+YCj4vIWHX/H+m+fyDOFdIYoAL4s4h8XFV/H8+43D6zgc+r6goR+RnwdeDb8YpJRN4HHFbV1SKyoKtxRDuuoON4WZod05iiLUJcvwLuBdT988fAv7Z5vy+f96QT7/GcRH3gfLgPAQUdvH4O8I+g53cBd/kYzwvARUHPdwB5bfp8CHgo6PlNwC99/jl5iWsYUBL0/ALguTjH9J84V5QlwEGgBvh9vH9Wbnsa8A/gDj/j6cTPKqaf9RAxFgIbQ7TH/POeiA8bTunYJcAWVd3XwesrgQkiMkZE0nFqHTztYzx/xbkJhYhMxLnB1Laq2x5gnohkueP5F+OMqfopYlyqehDYKyJnuE0XA8VxjukuVS1Q1UKcv7tXVfXjPsbkKS737y3i0uxYxkTsP+uISH7Q0/cDG0N0i8fnPfHE+7dIoj6A3wKfadM2HPh70PMrcWYP7MAZhvEznnTg9zgf5jXAwg5iugfY4vZ7FHfWQQLENRNYBazHSRwD4x1TUP8FxGZ2SsS4gPNxhhDWA2vdx5Xx/lnF8rPunu9RYIP7c3gayO8grph+3hPxYcvujTEmidlwijHGJDFL4sYYk8QsiRtjTBKzJG6MMUnMkrgxxiQxS+K9kIhUh2j7jIjcFOM4XnMr4613q+jdLyI5Qa+/HYVzFInI/3TyPQ+KyOTunrvNMXNE5HNhXn9YRA6LSKj50MZ0yKYY9kIiUq2q/WJ8TsH5vDUHtb2GU6lulbuI5D+BIlW9MErnTFXVxmgcq7vcOijPqurUDl6fD1QDv+uojzGh2JW4AVrrN3/Z/fo1EblPRN4Rka0icoHbniIiPxSRle7V86fd9n4i8oqIrBGRDSJyrdte6F5p/w5nMcbIjs6vTnW8rwKjRGSG+/5q9898EXlDnLrSG4PiWeSec52IvBL0fTwqIm8Bj4rIAnFrhbuvPSIiy0Rkt4h8QET+y435BXF2Im/5/otaYhCR/3DPsVxEhrrtV4tTNOpdEXk5qP1u96r6NRHZKSJfcL/FHwDj3O/hhyG+/zeAo139+zO9lyVx05FUVT0LuB34rtt2C06luLk4xZJuFZExwEng/ao6G2cJ94/dK2+ACTj1LKao6u5wJ1TVJpxa1ZPavPQxnNodM4EZwFoRyQMeAD6oqjNw6mi0mAxcoqofDXGaccBC4BqclYpLVXUaUItTlratvsBy9xxvALe67W8C81R1Fk5p1q8GvWcScDlOCdfvur8cvg7sUNWZqvqVcD8HYzqj11YxNBE95f65GqcAEcBlwHQRud59PgAnSe8Dvu8OCTTjlCkd6vbZrarLO3FeCdG2EnjYTYZ/VdW14lQefENVdwGoavBV7NOqWtvB8Z9X1QYR2YCz2cELbvuGoO8zWD3QsuvPauBS9+sC4E9ujY90nM0JWjynTn3uOhE5zKmfhTFRZ1fipiN17p9NnPplLzjlZGe6jzHq1G++AcgD5rhXy4eAlm2yTng9oYikANNoU8TIHWqYD+wHfuvhBmy4c7ZsftAMNOipm0LNhL6oCe4T/LP4OXC/exX/aU59v63nCPEeY6LOkrjpjH8Anw0aO54ozuYOA3BqczeIyEXA6M4e2D3mfwJ7VXV9m9dG42zO8QDOLjyzgeXAfHc4BxEZ1I3vqysG4PxSAfiEh/5VONutGRNVdoXQO2WJSHCJXa8lTx/EGXJY4455lwHXAY8Bz7hDFKtwqsp59ZiI1AEZOHs9htr2awHwFRFpwJnBcZOqlonIYuApEQkAhzk11BELd+NsQnAMeBVnY4IOqeoREXnLnUL4fNtxcRH5P5zvM9f9u/muqj7kS+SmR7EphsYYk8RsOMUYY5KYJXFjjElilsSNMSaJWRI3xpgkZkncGGOSmCVxY4xJYpbEjTEmif0/9qHYOUPI4DsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_project_test = ldaModel.transform(x_test)\n",
    "print(\"Shape of X:\", y_project_test.shape)\n",
    "print(\"Shape of transformed X:\", y_project_test.shape)\n",
    "\n",
    "x1, x2 = y_project_test[:, 0], y_project_test[:, 1]\n",
    "\n",
    "plt.scatter(\n",
    "    x1, x2, c=y_test, edgecolor=\"none\", alpha=0.8, cmap=plt.cm.get_cmap(\"viridis\", 3)\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Linear Discriminant 1\")\n",
    "plt.ylabel(\"Linear Discriminant 2\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree prediction result are described below.\n",
    "\n",
    "The accuracy is 75%, which is much higher than 0% from logistic regression. The reason it was higher is because decision tree has better performance on extreme cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [ 0,  0,  0],\n",
       "       [ 4,  5, 27]], dtype=int64)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test_dtclf = dtclf.predict(x_test)\n",
    "\n",
    "confusion_matrix(y_true=y_test, y_pred=y_pred_test_dtclf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_pred=y_pred_test_dtclf, y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost with Decision Tree as a base estimator will result 0 in accuracy. Because none of the test cases including class=1. This AdaBoost model did not work well on class=3 prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test_adaclf = adaclf.predict(x_test)\n",
    "\n",
    "accuracy(y_true=y_test, y_pred=y_pred_test_adaclf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youyu\\AppData\\Local\\Temp/ipykernel_4248/3908507820.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return round(np.sum(y_pred==y_true)/len(y_true),4)\n",
      "C:\\Users\\youyu\\AppData\\Local\\Temp/ipykernel_4248/3908507820.py:2: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  return round(np.sum(y_pred==y_true)/len(y_true),4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test_ab = ab.predict(x_test)\n",
    "\n",
    "accuracy(y_true=y_test, y_pred=y_pred_test_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Do you see any overfitting and underfitting issues on each model? How to overcome the overfitting/underfitting issues in your custom models? Provide the information model by model. (10 points)\n",
    "\n",
    "Yes, all models, except AdaBoost, have overfitting issues. It was mainly because the data split part did not separate data and classes well. Thus the training dataset does not include class=3 cases, and training process does not have records for class=3. When we predict the testing data with training records, it would be hard to find a good match and classification. \n",
    "\n",
    "In order to overcome this issue, we can re-split the original data again, and make sure training data includes similar cases in testing data. It means the training data should include more cases when class=3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) List down the important features in each model? How did you choose those important features? Why is it an important feature of the model? Explain with some statistical evidence. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Logistic regression, we can see the index of the largest weight in 3 categories. Since column 4 (Acl) are the most important feature in both class=2 and class=3, we can decide that this column is the most important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015055070642947577\n",
      "12\n",
      "0.09206366291483811\n",
      "4\n",
      "0.007414943976237376\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(lr.weights[i].max())\n",
    "    print(lr.weights[i].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Acl'"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is aiming to separate the groups as far as possible. In order to divide the classifications, the difference of the most important feature will be focused. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Alcohol', 'Malic.acid', 'Ash', 'Acl', 'Mg', 'Phenols', 'Flavanoids',\n",
       "       'Nonflavanoid.phenols', 'Proanth', 'Color.int', 'Hue', 'OD', 'Proline'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most important feature of LDA is Index(['Phenols'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "lda_feature_ind = np.where(ldaModel.linear_discriminants[0] == ldaModel.linear_discriminants[0].max())\n",
    "\n",
    "print(f\"The most important feature of LDA component 1 is {df.columns[1:][lda_feature_ind]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most important feature of LDA component 2 is Index(['Ash'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "lda_feature_ind2 = np.where(ldaModel.linear_discriminants[1] == ldaModel.linear_discriminants[1].max())\n",
    "\n",
    "print(f\"The most important feature of LDA component 2 is {df.columns[1:][lda_feature_ind2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For decision tree classifier, the most important feature is OD because it's the root feature. The method to calculate each root is to find the most important feature and the threshold of the feature to make the best split. Thus the root and its threshold is the most important one. \n",
    "\n",
    "Also, the best split was made based on the information gain. The information gain is calculated through the parent info gain - weighted child info gain. This step guaranteed that the root split made the largest information gain reduced among all the features and all the possible split ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OD'"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[dtclf.root.feature_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_12 <= 750.0 ? 0.3473538756391141\n",
      " left:X_6 <= 1.22 ? 0.1489821326061997\n",
      "  left:X_9 <= 3.27 ? 0.3550295857988165\n",
      "    left:2.0\n",
      "    right:3.0\n",
      "  right:X_11 <= 1.29 ? 0.028541147721366282\n",
      "    left:3.0\n",
      "    right:X_0 <= 13.11 ? 0.018365472910927313\n",
      "        left:2.0\n",
      "        right:X_0 <= 13.24 ? 0.4444444444444444\n",
      "                left:1.0\n",
      "                right:2.0\n",
      " right:X_9 <= 3.35 ? 0.11866231296422554\n",
      "  left:2.0\n",
      "  right:X_5 <= 1.4 ? 0.03388822829964333\n",
      "    left:3.0\n",
      "    right:1.0\n"
     ]
    }
   ],
   "source": [
    "dtclf.print_tree(tree=dtclf.root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost classifier has the same most important feature as Decision Tree Classifier, because AdaBoost use Decision Tree Classifier as the base estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OD\n",
      "OD\n",
      "OD\n"
     ]
    }
   ],
   "source": [
    "for i in range(ab.M):\n",
    "    print(df.columns[ab.G_M[i].root.feature_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82f6ad5c16f96bd4c09885635dc45a9905d4ac7b1c081542d45b0fe5279d5fc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
