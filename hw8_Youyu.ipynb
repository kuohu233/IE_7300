{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>IE 7300: Statistical Learning for Engineering</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>HW 8</center>\n",
    "<center>Youyu Zhang</center>\n",
    "<center>zhang.youy@northeastern.edu</center>\n",
    "<center>(530)574-2826</center>\n",
    "<center>Code available on: https://github.com/kuohu233/IE_7300</center>\n",
    "<center>Submitted by 11/15/2022</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports ##\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Any\n",
    "from abc import ABC,abstractmethod\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create custom classification models using the Bank Marketing dataset, https://archive-beta.ics.uci.edu/dataset/222/bank+marketing Links to an external site., and evaluate your model results. Split the dataset into training and test datasets 80:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup column names\n",
    "column_names = []\n",
    "for i in str(list(df.columns)).split(';'):\n",
    "    column_names.append(i[1:-1])\n",
    "column_names[0] = 'age'\n",
    "column_names[-1] = 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>day</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>...</th>\n",
       "      <th>month_jun</th>\n",
       "      <th>month_mar</th>\n",
       "      <th>month_may</th>\n",
       "      <th>month_nov</th>\n",
       "      <th>month_oct</th>\n",
       "      <th>month_sep</th>\n",
       "      <th>poutcome_failure</th>\n",
       "      <th>poutcome_other</th>\n",
       "      <th>poutcome_success</th>\n",
       "      <th>poutcome_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1787</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>4789</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>339</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1350</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>330</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1476</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>199</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  default  balance  housing  loan  day  duration  campaign  pdays  \\\n",
       "0   30        0     1787        0     0   19        79         1     -1   \n",
       "1   33        0     4789        1     1   11       220         1    339   \n",
       "2   35        0     1350        1     0   16       185         1    330   \n",
       "3   30        0     1476        1     1    3       199         4     -1   \n",
       "4   59        0        0        1     0    5       226         1     -1   \n",
       "\n",
       "   previous  ...  month_jun  month_mar  month_may  month_nov  month_oct  \\\n",
       "0         0  ...          0          0          0          0          1   \n",
       "1         4  ...          0          0          1          0          0   \n",
       "2         1  ...          0          0          0          0          0   \n",
       "3         0  ...          1          0          0          0          0   \n",
       "4         0  ...          0          0          1          0          0   \n",
       "\n",
       "   month_sep  poutcome_failure  poutcome_other  poutcome_success  \\\n",
       "0          0                 0               0                 0   \n",
       "1          0                 1               0                 0   \n",
       "2          0                 1               0                 0   \n",
       "3          0                 0               0                 0   \n",
       "4          0                 0               0                 0   \n",
       "\n",
       "   poutcome_unknown  \n",
       "0                 1  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 1  \n",
       "4                 1  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split and add data\n",
    "data_dict = {}\n",
    "for i in column_names:\n",
    "    data_dict[i] = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    item = df.iloc[i,0].split(';')\n",
    "    for j in range(len(item)):\n",
    "        data_dict[column_names[j]].append(item[j])\n",
    "\n",
    "data = pd.DataFrame(data=data_dict, columns=column_names)\n",
    "\n",
    "# Convert type of each column\n",
    "convert_dict = {'age':int, 'job':str, 'marital':str, 'education':str,\n",
    "                'default':str, 'balance':int, 'housing':str, 'loan':str,\n",
    "                'contact':str, 'day':int, 'month':str, 'duration':int,\n",
    "                'campaign':int, 'pdays':int,'previous':int,\n",
    "                'poutcome':str,'y':str}\n",
    "data = data.astype(convert_dict)\n",
    "\n",
    "data.default = pd.Series(np.where(data.default.values == '\"yes\"', 1, 0), data.index)\n",
    "data.housing = pd.Series(np.where(data.housing.values == '\"yes\"', 1, 0), data.index)\n",
    "data.loan = pd.Series(np.where(data.loan.values == '\"yes\"', 1, 0), data.index)\n",
    "data.y = pd.Series(np.where(data.y.values == '\"yes\"', 1, 0), data.index)\n",
    "\n",
    "# Dummy \n",
    "cat_vars = ['job','marital','education','contact','month', 'poutcome']\n",
    "\n",
    "for j in cat_vars:\n",
    "    var = []\n",
    "    for i in range(data.shape[0]):\n",
    "        var.append(data[j][i][1:-1])\n",
    "    data[j] = var\n",
    "\n",
    "for j in cat_vars:\n",
    "    dummies = pd.get_dummies(data[j],prefix=j)\n",
    "    data = pd.concat([data, dummies], axis='columns')\n",
    "    data = data.drop([j], axis='columns')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4521, 49)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and testing dataset. \n",
    "x_all = data.drop(['y'], axis=1)\n",
    "y_all = data['y']\n",
    "x_train, x_test = np.split(x_all,[int(0.8*len(x_all))])\n",
    "y_train, y_test = np.split(y_all,[int(0.8*len(y_all))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (B)\n",
    "Create an SVM and Knn model. Fit the model using the training dataset, and find the model accuracy and confusion matrix. Explain each model's outcome and accuracy. (10 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN(ABC):\n",
    "    \"\"\"\n",
    "    Base class for KNN implementations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, K : int = 3, metric : str = 'minkowski', p : int = 2) -> None:\n",
    "        \"\"\"\n",
    "        Initializer function. Ensure that input parameters are compatiable.\n",
    "        Inputs:\n",
    "            K      -> integer specifying number of neighbours to consider\n",
    "            metric -> string to indicate the distance metric to use (valid entries are 'minkowski' or 'cosine')\n",
    "            p      -> order of the minkowski metric (valid only when distance == 'minkowski')\n",
    "        \"\"\"\n",
    "        # check distance is a valid entry\n",
    "        valid_distance = ['minkowski','cosine']\n",
    "        if metric not in valid_distance:\n",
    "            msg = \"Entered value for metric is not valid. Pick one of {}\".format(valid_distance)\n",
    "            raise ValueError(msg)\n",
    "        # check minkowski p parameter\n",
    "        if (metric == 'minkowski') and (p <= 0):\n",
    "            msg = \"Entered value for p is not valid. For metric = 'minkowski', p >= 1\"\n",
    "            raise ValueError(msg)\n",
    "        # store/initialise input parameters\n",
    "        self.K       = K\n",
    "        self.metric  = metric\n",
    "        self.p       = p\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        \n",
    "    def __del__(self) -> None:\n",
    "        \"\"\"\n",
    "        Destructor function. \n",
    "        \"\"\"\n",
    "        del self.K\n",
    "        del self.metric\n",
    "        del self.p\n",
    "        del self.X_train\n",
    "        del self.y_train\n",
    "      \n",
    "    def __minkowski(self, x : np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Private function to compute the minkowski distance between point x and the training data X\n",
    "        Inputs:\n",
    "            x -> numpy data point of predictors to consider\n",
    "        Outputs:\n",
    "            np.array -> numpy array of the computed distances\n",
    "        \"\"\"\n",
    "        return np.power(np.sum(np.power(np.abs(self.X_train - x),self.p),axis=1),1/self.p)\n",
    "    \n",
    "    def __cosine(self, x : np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Private function to compute the cosine distance between point x and the training data X\n",
    "        Inputs:\n",
    "            x -> numpy data point of predictors to consider\n",
    "        Outputs:\n",
    "            np.array -> numpy array of the computed distances\n",
    "        \"\"\"\n",
    "        return (1 - (np.dot(self.X_train,x)/(np.linalg.norm(x)*np.linalg.norm(self.X_train,axis=1))))\n",
    "    \n",
    "    def __distances(self, X : np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Private function to compute distances to each point x in X[x,:]\n",
    "        Inputs:\n",
    "            X -> numpy array of points [x]\n",
    "        Outputs:\n",
    "            D -> numpy array containing distances from x to all points in the training set.\n",
    "        \"\"\"\n",
    "        # cover distance calculation\n",
    "        if self.metric == 'minkowski':\n",
    "            D = np.apply_along_axis(self.__minkowski,1,X)\n",
    "        elif self.metric == 'cosine':\n",
    "            D = np.apply_along_axis(self.__cosine,1,X)\n",
    "        # return computed distances\n",
    "        return D\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _generate_predictions(self, idx_neighbours : np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Protected function to compute predictions from the K nearest neighbours\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X : np.array, y : np.array) -> None:\n",
    "        \"\"\"\n",
    "        Public training function for the class. It is assummed input X has been normalised.\n",
    "        Inputs:\n",
    "            X -> numpy array containing the predictor features\n",
    "            y -> numpy array containing the labels associated with each value in X\n",
    "        \"\"\"\n",
    "        # store training data\n",
    "        self.X_train = np.copy(X)\n",
    "        self.y_train = np.copy(y)\n",
    "        \n",
    "    def predict(self, X : np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Public prediction function for the class. \n",
    "        It is assummed input X has been normalised in the same fashion as the input to the training function\n",
    "        Inputs:\n",
    "            X -> numpy array containing the predictor features\n",
    "        Outputs:\n",
    "           y_pred -> numpy array containing the predicted labels\n",
    "        \"\"\"\n",
    "        # ensure we have already trained the instance\n",
    "        if (self.X_train.size == 0) or (self.y_train.size == 0):\n",
    "            raise Exception('Model is not trained. Call fit before calling predict.')\n",
    "        # compute distances\n",
    "        D = self.__distances(X)\n",
    "        # obtain indices for the K nearest neighbours\n",
    "        idx_neighbours = D.argsort()[:,:self.K]\n",
    "        # compute predictions\n",
    "        y_pred = self._generate_predictions(idx_neighbours)\n",
    "        # return results\n",
    "        return y_pred\n",
    "    \n",
    "    def get_params(self, deep : bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Public function to return model parameters\n",
    "        Inputs:\n",
    "            deep -> boolean input parameter\n",
    "        Outputs:\n",
    "            Dict -> dictionary of stored class input parameters\n",
    "        \"\"\"\n",
    "        return {'K':self.K,\n",
    "                'metric':self.metric,\n",
    "                'p':self.p}\n",
    "\n",
    "\n",
    "class KNNClassifier(KNN):\n",
    "    \"\"\"\n",
    "    Class for KNN classifiction implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, K : int = 3, metric : str = 'minkowski', p : int = 2) -> None:\n",
    "        \"\"\"\n",
    "        Initializer function. Ensure that input parameters are compatiable.\n",
    "        Inputs:\n",
    "            K       -> integer specifying number of neighbours to consider\n",
    "            metric  -> string to indicate the distance metric to use (valid entries are 'minkowski' or 'cosine')\n",
    "            p       -> order of the minkowski metric (valid only when distance == 'minkowski')\n",
    "        \"\"\"\n",
    "        # call base class initialiser\n",
    "        super().__init__(K,metric,p)\n",
    "        \n",
    "    def _generate_predictions(self, idx_neighbours : np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Protected function to compute predictions from the K nearest neighbours\n",
    "        Inputs:\n",
    "            idx_neighbours -> indices of nearest neighbours\n",
    "        Outputs:\n",
    "            y_pred -> numpy array of prediction results\n",
    "        \"\"\"        \n",
    "        # compute the mode label for each submitted sample\n",
    "        y_pred = stats.mode(self.y_train[idx_neighbours],axis=1).mode.flatten()   \n",
    "        # return result\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNClassifier()\n",
    "knn.fit(np.array(x_train),np.array(y_train))\n",
    "y_pred_knn_train = knn.predict(np.array(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix below inidicates that most of the cases are correctly classified. The accuracy of the model in training dataset is 92.1%. \n",
    "\n",
    "The prediction accuracy was not good when y=yes (row 1). More than 50% of the prediction was false to be y=no. This could happen because of the unbalanced training data sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3140,   66],\n",
       "       [ 221,  189]], dtype=int64)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_true=np.array(y_train), y_pred=y_pred_knn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9206"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    return round(np.sum(y_pred==y_true)/len(y_true),4)\n",
    "\n",
    "accuracy(y_pred=y_pred_knn_train, y_true=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (\n",
    "                        2 * self.lambda_param * self.w - np.dot(x_i, y_[idx])\n",
    "                    )\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVM()\n",
    "svm.fit(np.array(x_train),np.array(y_train))\n",
    "y_pred_svm_train = svm.predict(np.array(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy of SVM is 88.1%, a little bit lower than that of KNN. The confusion matrix below indicate the misclassification cases of y=yes is twice more than that of y=no. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3161,   45],\n",
       "       [ 384,   26]], dtype=int64)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_train_svm = pd.DataFrame(data={'train':y_train, 'pred':pd.Series(np.where(y_pred_svm_train == 1.0, 1, 0))})\n",
    "confusion_matrix(y_true=np.array(y_train), y_pred=result_train_svm['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8814"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_pred=result_train_svm['pred'], y_true=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (C) \n",
    "Compare the Knn model with a few K values and find the best k model. Describe each model performance (10 points)\n",
    "\n",
    "The accuracy result indicates that k=3 will give highest accuracy in training dataset. \n",
    "The confusion matrix also prove that k=3 will give least misclassification in cases where y=yes.\n",
    "As k increases over 3, the accuracy drops slowly, and thus I would recommend a model with k=3. \n",
    "\n",
    "For k=2, the prediction on cases where y=no (row 0) reaches 100% correct. Meanwhile the cases where y=yes was not fully predicted. \n",
    "\n",
    "For k=4, the misclassification cases increases on both conditions, and similarly to k=5. The overall accuracy of the two models are not compatible to k=3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3206    0]\n",
      " [ 298  112]]\n",
      "[[3140   66]\n",
      " [ 221  189]]\n",
      "[[3183   23]\n",
      " [ 316   94]]\n",
      "[[3144   62]\n",
      " [ 274  136]]\n"
     ]
    }
   ],
   "source": [
    "test_k = [2, 3, 4, 5]\n",
    "test_acc = []\n",
    "for i in test_k:\n",
    "    knn_test = KNNClassifier(i)\n",
    "    knn_test.fit(np.array(x_train),np.array(y_train))\n",
    "    y_pred_knn_train_test = knn_test.predict(np.array(x_train))\n",
    "    test_acc.append(accuracy(y_pred=y_pred_knn_train_test, y_true=y_train))\n",
    "    print(confusion_matrix(y_true=np.array(y_train), y_pred=y_pred_knn_train_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9176, 0.9206, 0.9062, 0.9071]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your SVM model with various kernel methods (linear, rbf, and polynomial). Explain each Kernal parameter (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "class KernelSvmClassifier:\n",
    "    \n",
    "    def __init__(self, C, kernel):\n",
    "        self.C = C                               \n",
    "        self.kernel = kernel          # <---\n",
    "        self.alpha = None\n",
    "        self.supportVectors = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        N = len(y)\n",
    "        # --->\n",
    "        # Gram matrix of h(x) y\n",
    "        hXX = np.apply_along_axis(lambda x1 : np.apply_along_axis(lambda x2:  self.kernel(x1, x2), 1, X),\n",
    "                                  1, X)   \n",
    "        yp = y.reshape(-1, 1)\n",
    "        GramHXy = hXX * np.matmul(yp, yp.T) \n",
    "        # <---\n",
    "\n",
    "        # Lagrange dual problem\n",
    "        def Ld0(G, alpha):\n",
    "            return alpha.sum() - 0.5 * alpha.dot(alpha.dot(G))\n",
    "\n",
    "        # Partial derivate of Ld on alpha\n",
    "        def Ld0dAlpha(G, alpha):\n",
    "            return np.ones_like(alpha) - alpha.dot(G)\n",
    "\n",
    "        # Constraints on alpha of the shape :\n",
    "        # -  d - C*alpha  = 0\n",
    "        # -  b - A*alpha >= 0\n",
    "        A = np.vstack((-np.eye(N), np.eye(N)))             # <---\n",
    "        b = np.hstack((np.zeros(N), self.C * np.ones(N)))  # <---\n",
    "        constraints = ({'type': 'eq',   'fun': lambda a: np.dot(a, y),     'jac': lambda a: y},\n",
    "                       {'type': 'ineq', 'fun': lambda a: b - np.dot(A, a), 'jac': lambda a: -A})\n",
    "\n",
    "        # Maximize by minimizing the opposite\n",
    "        optRes = optimize.minimize(fun=lambda a: -Ld0(GramHXy, a),\n",
    "                                   x0=np.ones(N), \n",
    "                                   method='SLSQP', \n",
    "                                   jac=lambda a: -Ld0dAlpha(GramHXy, a), \n",
    "                                   constraints=constraints)\n",
    "        self.alpha = optRes.x\n",
    "        # --->\n",
    "        epsilon = 1e-8\n",
    "        supportIndices = self.alpha > epsilon\n",
    "        self.supportVectors = X[supportIndices]\n",
    "        self.supportAlphaY = y[supportIndices] * self.alpha[supportIndices]\n",
    "        # <---\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict y values in {-1, 1} \"\"\"\n",
    "        # --->\n",
    "        def predict1(x):\n",
    "            x1 = np.apply_along_axis(lambda s: self.kernel(s, x), 1, self.supportVectors)\n",
    "            x2 = x1 * self.supportAlphaY\n",
    "            return np.sum(x2)\n",
    "        \n",
    "        d = np.apply_along_axis(predict1, 1, X)\n",
    "        return 2 * (d > 0) - 1\n",
    "        # <---\n",
    "\n",
    "def GRBF(x1, x2):\n",
    "    diff = x1 - x2\n",
    "    return np.exp(-np.dot(diff, diff) * len(x1) / 2)\n",
    "\n",
    "def poly(x1, x2):\n",
    "    return np.dot(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of grbf kernel in SVM doesn't look good. The overall accuracy is 88.7%, which is lower than all the previous knn models. All the y-yes cases are misclassified which means this model doesn't work well on this condition. It also take much longer time in fitting than other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youyu\\AppData\\Local\\Temp/ipykernel_9672/1102196909.py:65: RuntimeWarning: overflow encountered in int_scalars\n",
      "  return np.exp(-np.dot(diff, diff) * len(x1) / 2)\n",
      "C:\\Users\\youyu\\AppData\\Local\\Temp/ipykernel_9672/1102196909.py:65: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(-np.dot(diff, diff) * len(x1) / 2)\n",
      "C:\\Users\\youyu\\AppData\\Local\\Temp/ipykernel_9672/1102196909.py:18: RuntimeWarning: invalid value encountered in multiply\n",
      "  GramHXy = hXX * np.matmul(yp, yp.T)\n"
     ]
    }
   ],
   "source": [
    "svm_grbf = KernelSvmClassifier(C=1, kernel=GRBF)\n",
    "svm_grbf.fit(np.array(x_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youyu\\AppData\\Local\\Temp/ipykernel_9672/1102196909.py:65: RuntimeWarning: overflow encountered in int_scalars\n",
      "  return np.exp(-np.dot(diff, diff) * len(x1) / 2)\n",
      "C:\\Users\\youyu\\AppData\\Local\\Temp/ipykernel_9672/1102196909.py:65: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(-np.dot(diff, diff) * len(x1) / 2)\n",
      "C:\\Users\\youyu\\AppData\\Local\\Temp/ipykernel_9672/1102196909.py:56: RuntimeWarning: invalid value encountered in multiply\n",
      "  x2 = x1 * self.supportAlphaY\n"
     ]
    }
   ],
   "source": [
    "y_pred_svmgrbf_train = svm_grbf.predict(np.array(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3206,    0],\n",
       "       [ 410,    0]], dtype=int64)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_train_svmgrbf = pd.DataFrame(data={'train':y_train, 'pred':pd.Series(np.where(y_pred_svmgrbf_train == 1.0, 1, 0))})\n",
    "confusion_matrix(y_true=np.array(y_train), y_pred=result_train_svmgrbf['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8866"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_pred=result_train_svmgrbf['pred'],y_true=np.array(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial kernel looks to have the same prediction as grbf got before. The reason is that the polynomial was not working or the kernel function is not correctly used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly = KernelSvmClassifier(C=1, kernel=poly)\n",
    "svm_poly.fit(np.array(x_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svmpoly_train = svm_poly.predict(np.array(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3206,    0],\n",
       "       [ 410,    0]], dtype=int64)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_train_svmpoly = pd.DataFrame(data={'train':y_train, 'pred':pd.Series(np.where(y_pred_svmpoly_train == 1.0, 1, 0))})\n",
    "confusion_matrix(y_true=np.array(y_train), y_pred=result_train_svmpoly['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8866"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_pred=result_train_svmpoly['pred'],y_true=np.array(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of SVM in linear kernel looks similar to grbf, but the misclassification cases distribution is not. Both conditions have misclassification, and evenly distributed corresponding to their proportions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3161,   45],\n",
       "       [ 384,   26]], dtype=int64)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_train_svm = pd.DataFrame(data={'train':y_train, 'pred':pd.Series(np.where(y_pred_svm_train == 1.0, 1, 0))})\n",
    "confusion_matrix(y_true=np.array(y_train), y_pred=result_train_svm['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8814"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_pred=result_train_svm['pred'], y_true=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (D) \n",
    "Analyze your SVM and Knn model performance with outlier and imbalanced samples (make a sample from the given dataset). \n",
    "\n",
    "How do the SVM and Knn models handle the outliers and imbalanced datasets with statistical evidence? (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sampling\n",
    "subset1 = data[data['y']==1][0:10]\n",
    "subset2 = data[data['y']==0][0:10]\n",
    "subset = pd.concat([subset1, subset2])\n",
    "\n",
    "x_sample = subset.drop(['y'], axis=1)\n",
    "y_sample = subset['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_acc = []\n",
    "knn_sample = KNNClassifier(3)\n",
    "knn_sample.fit(np.array(x_sample),np.array(y_sample))\n",
    "y_pred_knn_train_sample = knn_sample.predict(np.array(x_sample))\n",
    "sample_acc.append(accuracy(y_pred=y_pred_knn_train_sample, y_true=y_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to balance the previous data, I subset the original dataset and chose samples evenly based on the situations of Y. A small sample dataset of 20 cases were used here. Both y=no and y=yes have 10 cases. \n",
    "\n",
    "The confusion matrix and accuracy below indicated much lower efficiency in this knn model. The misclassification of both conditions are the same and both are lower than previous model. The overall accuracy is 80% and it can be worse in test dataset instead of training dataset. Larger dataset is needed for KNN model in order to provide better prediction and accuracy. \n",
    "\n",
    "In KNN, prediction of a single case is made by the nearby k cases. In order to get the nearby k points, distances are calculated, and the outliers means the points that are far away from the other points, and thus the accuracy of the outliers will drop. If the data subset is chosen based on the x_train similarity, the accuracy can be higher in the training dataset (the accuracy of testing dataset cannot be guaranteed because the dataset size is smaller)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 2]\n",
      " [2 8]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true=np.array(y_sample), y_pred=y_pred_knn_train_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8]\n"
     ]
    }
   ],
   "source": [
    "print(sample_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to SVM model. The chosen small sample cannot reach the accuracy in the previous trials. The confusion matrix cannot easily tell if the prediction is correct or wrong because all the situations are alike. That means a large partion of the cases are wrongly classified. The accuracy of 70% is the lowest among all the models until now. \n",
    "\n",
    "The outliers are not that important to SVM compared to KNN because SVM is designed to minimized the distances from the hyperplane to the data points of each group meanwhile maximize the distances between the groups. In the current subset, the distance beween groups are not large enough to maximize the soft margin, and thus the result and accuracy looks not good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_sample = SVM()\n",
    "svm_sample.fit(np.array(x_sample),np.array(y_sample))\n",
    "y_pred_svm_sample = svm_sample.predict(np.array(x_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 2],\n",
       "       [4, 6]], dtype=int64)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_sample_svm = pd.DataFrame(data={'train':np.array(y_sample), 'pred':pd.Series(np.where(y_pred_svm_sample == 1.0, 1, 0))})\n",
    "confusion_matrix(y_true=np.array(y_sample), y_pred=result_sample_svm['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_true=np.array(y_sample), y_pred=result_sample_svm['pred'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82f6ad5c16f96bd4c09885635dc45a9905d4ac7b1c081542d45b0fe5279d5fc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
